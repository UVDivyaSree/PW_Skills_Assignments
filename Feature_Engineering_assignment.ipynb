{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1.What is a parameter?\n",
        "In machine learning, a parameter refers to a variable within a model that is learned or optimized during training. These parameters define the model's behavior and are adjusted to minimize the error between the model's predictions and the actual data.\n",
        "\n",
        "###Key Characteristics of Parameters:\n",
        "###Learned from Data:\n",
        " Parameters are updated during the training process using algorithms like gradient descent.\n",
        "\n",
        "###Define the Model's Output:\n",
        "They directly influence the predictions the model makes.\n",
        "\n",
        "###Dependent on the Model:\n",
        "Different types of models have different parameters.\n",
        "\n",
        "###For example:\n",
        "###Linear Regression:\n",
        " Parameters are the coefficients (slopes) and intercept of the line.\n",
        "\n",
        "###Neural Networks:\n",
        "Parameters are the weights and biases of the network.\n",
        "\n",
        "###Examples of Parameters:\n",
        "###In a linear regression model:\n",
        "ùë¶\n",
        "=\n",
        "m\n",
        "ùë•\n",
        "+\n",
        "c\n",
        "\n",
        "###In a neural network:\n",
        "Weights: Control the strength of the connection between nodes.\n",
        "\n",
        "Biases: Shift the activation function output.\n",
        "\n",
        "###Parameters vs. Hyperparameters:\n",
        "Parameters: Learned by the model during training (e.g., weights, biases).\n",
        "\n",
        "Hyperparameters: Set by the user and not learned during training (e.g., learning rate, number of layers in a neural network).\n",
        "\n",
        "Understanding and optimizing parameters is crucial for building effective machine-learning models.\n",
        "\n"
      ],
      "metadata": {
        "id": "HYqYg5u9QCgS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.What is correlation? What does negative correlation mean?\n",
        "\n",
        "Correlation is a statistical measure that describes the strength and direction of a relationship between two variables. It helps in understanding whether and how strongly variables are related.\n",
        "\n",
        "###Positive Correlation:\n",
        "When one variable increases, the other variable also increases.\n",
        "\n",
        "###Negative Correlation:\n",
        " When one variable increases, the other variable decreases.\n",
        "\n",
        "###No Correlation:\n",
        "When there is no consistent relationship between the variables.\n",
        "\n",
        "The correlation is typically represented by the correlation coefficient (\n",
        "ùëü), which ranges from\n",
        "‚àí\n",
        "1\n",
        " to\n",
        "1\n",
        ":\n",
        "\n",
        "ùëü\n",
        "=\n",
        "1: Perfect positive correlation.\n",
        "\n",
        "ùëü\n",
        "=\n",
        "‚àí\n",
        "1: Perfect negative correlation.\n",
        "\n",
        "ùëü\n",
        "=\n",
        "0: No correlation.\n",
        "\n",
        "###Negative Correlation:\n",
        "Negative correlation indicates an inverse relationship between two variables. As one variable increases, the other decreases, and vice versa.\n",
        "\n",
        "###Examples:\n",
        "Real-world Example:\n",
        "Hours spent watching TV and exam scores. As hours spent watching TV increase, exam scores might decrease.\n",
        "\n",
        "Financial Example:\n",
        "Price of a product and its demand. As the price of a product increases, its demand typically decreases.\n",
        "\n",
        "Visual Representation:\n",
        "In a scatter plot, a negative correlation shows points trending downward from left to right.\n",
        "\n",
        "Interpreting the Strength of Correlation:\n",
        "\n",
        "‚àí\n",
        "1\n",
        "‚â§\n",
        "ùëü\n",
        "<\n",
        "‚àí\n",
        "0.7: Strong negative correlation.\n",
        "\n",
        "‚àí\n",
        "0.7\n",
        "‚â§\n",
        "ùëü\n",
        "<\n",
        "‚àí\n",
        "0.3: Moderate negative correlation.\n",
        "\n",
        "‚àí\n",
        "0.3\n",
        "‚â§\n",
        "ùëü\n",
        "<\n",
        "0: Weak negative correlation.\n",
        "\n",
        "Understanding correlation helps in exploring relationships between variables, which is crucial for predictive modeling and data analysis."
      ],
      "metadata": {
        "id": "aw6m-YcWQCj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "###Definition of Machine Learning (ML):\n",
        "\n",
        "Machine Learning is a subset of artificial intelligence (AI) that enables systems to learn and improve from experience without being explicitly programmed. It uses algorithms and statistical models to identify patterns in data and make predictions or decisions.\n",
        "\n",
        "###Main Components in Machine Learning:\n",
        "1.Data:\n",
        "\n",
        "The foundation of ML, as algorithms learn from data.\n",
        "\n",
        "Types: Structured (e.g., tables, databases) and unstructured (e.g., images, text, audio).\n",
        "\n",
        "Quality and quantity of data significantly affect model performance.\n",
        "\n",
        "2.Features (Input Variables):\n",
        "\n",
        "The attributes or characteristics of the data used by the model to make predictions.\n",
        "\n",
        "Feature engineering (e.g., selection, scaling, transformation) is crucial for improving model accuracy.\n",
        "\n",
        "3.Model:\n",
        "\n",
        "A mathematical representation of the real-world process you are trying to understand or predict.\n",
        "\n",
        "Examples: Linear regression, decision trees, neural networks.\n",
        "\n",
        "4.Algorithm:\n",
        "\n",
        "A set of rules or methods used to train the model on the data.\n",
        "\n",
        "Examples: Gradient Descent, Support Vector Machines (SVM), K-Nearest Neighbors (KNN).\n",
        "\n",
        "5.Training:\n",
        "\n",
        "The process where the model learns patterns from the data by minimizing the error (loss).\n",
        "\n",
        "Typically involves splitting the data into training and testing sets.\n",
        "\n",
        "6.Evaluation:\n",
        "\n",
        "Assessing the model's performance using metrics like accuracy, precision, recall, F1-score, and mean squared error.\n",
        "\n",
        "Often involves a validation dataset or techniques like cross-validation.\n",
        "\n",
        "7.Prediction/Inference:\n",
        "\n",
        "Once trained, the model is used to make predictions or decisions based on new, unseen data.\n",
        "\n",
        "8.Optimization:\n",
        "\n",
        "Adjusting model parameters to improve accuracy and reduce error.\n",
        "\n",
        "Includes techniques like hyperparameter tuning and regularization.\n",
        "\n",
        "9.Feedback Loop (Optional):\n",
        "\n",
        "Systems that improve continuously by incorporating new data (e.g., reinforcement learning).\n",
        "\n",
        "###The Workflow of Machine Learning:\n",
        "###1.Problem Definition:\n",
        " Understand the problem you aim to solve.\n",
        "\n",
        "###2.Data Collection:\n",
        " Gather data relevant to the problem.\n",
        "\n",
        "###3.Data Preprocessing:\n",
        "Clean and prepare data for analysis.\n",
        "\n",
        "###4.Feature Engineering:\n",
        " Select and transform features to improve model performance.\n",
        "\n",
        "###5.Model Selection:\n",
        " Choose an appropriate ML algorithm.\n",
        "\n",
        "###6.Training and Testing:\n",
        " Train the model and evaluate its performance.\n",
        "\n",
        "###7.Deployment:\n",
        " Use the model for predictions in real-world applications.\n",
        "\n",
        "Machine learning integrates these components to build intelligent systems capable of making decisions with minimal human intervention."
      ],
      "metadata": {
        "id": "zptnciHVSzOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "The loss value is a numerical representation of how well a machine learning model's predictions match the actual target values. It plays a crucial role in determining the quality of the model during training and optimization.\n",
        "\n",
        "###How the Loss Value Helps Evaluate a Model\n",
        "1.Quantifies Error:\n",
        "\n",
        "The loss function calculates the difference between the predicted outputs and the true values.\n",
        "\n",
        "A lower loss value indicates that the model's predictions are closer to the actual values, which means the model is performing better.\n",
        "\n",
        "2. Model Optimization:\n",
        "\n",
        "During training, algorithms like gradient descent use the loss value to adjust the model's parameters (weights and biases) to minimize error.\n",
        "\n",
        "By observing the loss value over epochs, we can see if the model is improving (loss decreasing) or struggling (loss stagnant or increasing).\n",
        "\n",
        "3.Choosing the Best Model:\n",
        "\n",
        "In scenarios where multiple models are trained, the model with the lowest loss on a validation or test dataset is usually considered the best.\n",
        "\n",
        "4.Overfitting/Underfitting Indicator:\n",
        "\n",
        "High training loss: Model is underfitting (not learning enough from the data).\n",
        "\n",
        "Low training loss but high validation/test loss: Model is overfitting (learning too much noise from the training data).\n",
        "\n",
        "###Types of Loss Functions\n",
        "The type of loss function used depends on the problem:\n",
        "\n",
        "1.Regression:\n",
        "Mean Squared Error (MSE), Mean Absolute Error (MAE)\n",
        "\n",
        "2.Classification:\n",
        "\n",
        "Cross-Entropy Loss, Hinge\n",
        "\n",
        "3.Custom Loss:\n",
        "\n",
        "Sometimes domain-specific losses are designed for specialized tasks.\n",
        "\n",
        "###How to Determine if the Model is Good Using Loss\n",
        "1.Compare Training and Validation Loss:\n",
        "\n",
        "If both are low and similar, the model generalizes well.\n",
        "\n",
        "If the training loss is low but validation loss is high, it indicates overfitting.\n",
        "\n",
        "2.Monitor the Trend:\n",
        "\n",
        "During training, loss should steadily decrease. Fluctuating or stagnant loss values might indicate problems like poor learning rate or insufficient data.\n",
        "\n",
        "3.Set a Baseline:\n",
        "\n",
        "Compare the model's loss to a baseline (e.g., random predictions or a simple heuristic). If the loss is significantly better, the model is effective.\n",
        "\n",
        "4.Domain-Specific Requirements:\n",
        "\n",
        "A \"good\" loss value depends on the context. For example, in some applications, a small error might be critical, while in others, higher tolerance is acceptable.\n",
        "\n",
        "In summary, the loss value serves as a feedback mechanism during training, providing critical insights into the model's performance and areas for improvement. A consistently low and stable loss on both training and validation data is a strong indicator of a good model."
      ],
      "metadata": {
        "id": "24YHQkv_QCnK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5.What are continuous and categorical variables?\n",
        "\n",
        "###Continuous and Categorical Variables\n",
        "In statistics and machine learning, variables represent different features or attributes of data. They are generally classified into continuous and categorical variables based on the nature of their values.\n",
        "\n",
        "###1. Continuous Variables\n",
        "A continuous variable can take an infinite number of values within a given range. These variables are typically numerical and can be measured but not counted.\n",
        "\n",
        "###Characteristics:\n",
        "Values are real numbers.\n",
        "Can take fractional or decimal values.\n",
        "Represent measurable quantities like height, weight, or temperature.\n",
        "\n",
        "###Examples:\n",
        "Height in centimeters (e.g., 172.5 cm).\n",
        "\n",
        "Temperature in degrees Celsius (e.g., 22.4¬∞C).\n",
        "\n",
        "Age in years (e.g., 25.8 years).\n",
        "\n",
        "###Usage in Machine Learning:\n",
        "Continuous variables are often used in regression tasks.\n",
        "\n",
        "Require normalization or standardization for models like neural networks or k-NN.\n",
        "\n",
        "###2. Categorical Variables\n",
        "A categorical variable has a finite set of distinct values or categories. These variables represent qualitative data and are often used to classify data into groups.\n",
        "\n",
        "###Characteristics:\n",
        "Values are discrete and cannot be divided into fractions.\n",
        "\n",
        "Represent labels or groups.\n",
        "\n",
        "Categories can be nominal (no inherent order) or ordinal (ordered).\n",
        "\n",
        "###Examples:\n",
        "Nominal:\n",
        "Colors (red, blue, green).\n",
        "\n",
        "Gender (male, female, other).\n",
        "\n",
        "Ordinal:\n",
        "Education level (high school, bachelor's, master's, Ph.D.).\n",
        "\n",
        "Customer satisfaction (poor, average, excellent).\n",
        "\n",
        "###Usage in Machine Learning:\n",
        "Used in classification tasks.\n",
        "\n",
        "Need to be encoded numerically (e.g., one-hot encoding, label encoding).\n",
        "\n"
      ],
      "metadata": {
        "id": "_l46v57zQCpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6.How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "###Handling Categorical Variables in Machine Learning\n",
        "\n",
        "Categorical variables must be transformed into numerical representations for most machine learning models to process them effectively. Several techniques are used depending on the nature of the variable and the problem at hand.\n",
        "\n",
        "###Common Techniques for Handling Categorical Variables\n",
        "###1. Label Encoding\n",
        "Assigns a unique integer to each category.\n",
        "Simple and efficient for variables with an ordinal relationship.\n",
        "Example:\n",
        "\n",
        "Categories: ['Low', 'Medium', 'High']\n",
        "Encoded: [0, 1, 2]\n",
        "\n",
        "Use Case: Ordinal variables with inherent order, e.g., education level.\n",
        "\n",
        "###2. One-Hot Encoding\n",
        "Creates binary (0 or 1) columns for each category, indicating its presence or absence.\n",
        "Useful for nominal (unordered) variables.\n",
        "Example:\n",
        "\n",
        "Categories: ['Red', 'Blue', 'Green']\n",
        "\n",
        "Encoded:\n",
        "```\n",
        "Red   Blue  Green\n",
        "1     0     0\n",
        "0     1     0\n",
        "0     0     1\n",
        "```\n",
        "Use Case: Nominal variables like colors, genders.\n",
        "\n",
        "Limitations:\n",
        "\n",
        "Can lead to the curse of dimensionality for high-cardinality variables (too many categories).\n",
        "\n",
        "###3. Ordinal Encoding\n",
        "Assigns integers to categories based on their rank or order.\n",
        "Example:\n",
        "\n",
        "Categories: ['Poor', 'Average', 'Good', 'Excellent']\n",
        "Encoded: [1, 2, 3, 4]\n",
        "\n",
        "Use Case: Ordinal variables with a clear ranking.\n",
        "\n",
        "###4. Target/Mean Encoding\n",
        "Replaces each category with the mean (or other statistical measure) of the target variable for that category.\n",
        "Captures the relationship between the category and the target variable.\n",
        "\n",
        "Example (for regression target):\n",
        "\n",
        "Categories: ['A', 'B', 'C']\n",
        "Target Means: [0.3, 0.6, 0.9]\n",
        "Encoded: [0.3, 0.6, 0.9]\n",
        "\n",
        "Use Case: High-cardinality categorical variables.\n",
        "\n",
        "Limitations:\n",
        "\n",
        "Can lead to data leakage if not handled properly during cross-validation.\n",
        "\n",
        "###5. Binary Encoding\n",
        "Combines label encoding and one-hot encoding by representing categories in binary form.\n",
        "Example:\n",
        "\n",
        "Categories: ['A', 'B', 'C', 'D']\n",
        "Encoded (Binary):\n",
        "```\n",
        "A ‚Üí 01\n",
        "B ‚Üí 10\n",
        "C ‚Üí 11\n",
        "```\n",
        "Use Case: Reduces dimensionality for high-cardinality variables.\n",
        "\n",
        "###6. Frequency Encoding\n",
        "Replaces each category with its frequency (count) in the dataset.\n",
        "Example:\n",
        "\n",
        "Categories: ['A', 'B', 'B', 'C', 'C', 'C']\n",
        "Encoded: [1, 2, 2, 3, 3, 3]\n",
        "\n",
        "Use Case: When the frequency of categories carries meaningful information.\n",
        "\n",
        "###7. Embedding Layers (Advanced)\n",
        "Used in deep learning models.\n",
        "\n",
        "Maps categories to dense, continuous vector representations learned during training.\n",
        "\n",
        "Use Case: High-cardinality variables in deep learning tasks, such as word embeddings in NLP.\n",
        "\n",
        "###Choosing the Right Technique\n",
        "1.Nature of the Variable:\n",
        "\n",
        "Nominal: One-hot encoding, binary encoding.\n",
        "Ordinal: Label encoding, ordinal encoding.\n",
        "\n",
        "2.Number of Categories:\n",
        "\n",
        "Few categories: One-hot encoding.\n",
        "Many categories: Target encoding, binary encoding, or embeddings.\n",
        "\n",
        "3.Model Type:\n",
        "\n",
        "Tree-based models (e.g., decision trees, random forests): Can handle label encoding directly.\n",
        "\n",
        "Linear models (e.g., logistic regression): Prefer one-hot encoding to avoid misleading ordinal relationships.\n",
        "\n",
        "4.Avoiding Data Leakage:\n",
        "\n",
        "Ensure techniques like target encoding or frequency encoding are applied only to the training set to prevent using information from the test set.\n",
        "\n",
        "Proper handling of categorical variables is essential to ensure models interpret them correctly and perform effectively."
      ],
      "metadata": {
        "id": "dRAssrbXQCxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.What do you mean by training and testing a dataset?\n",
        "\n",
        "###Training and Testing a Dataset\n",
        "In machine learning, the dataset is typically divided into two main subsets: training set and testing set. This division ensures that the model is trained and evaluated properly to generalize well to new, unseen data.\n",
        "\n",
        "###1. Training Dataset\n",
        "The training dataset is the portion of data used to train the machine learning model. It contains input features (independent variables) and corresponding target labels (dependent variables) that the model learns from.\n",
        "\n",
        "###Purpose:\n",
        "To allow the model to learn patterns, relationships, and dependencies in the data.\n",
        "The model adjusts its parameters (e.g., weights and biases) to minimize error (loss) during training.\n",
        "\n",
        "###Example:\n",
        "For a dataset of house prices:\n",
        "\n",
        "Input features: Number of bedrooms, square footage, location.\n",
        "\n",
        "Target label: Price of the house.\n",
        "\n",
        "The training data would include examples of houses with known features and prices.\n",
        "\n",
        "###2. Testing Dataset\n",
        "The testing dataset is a separate portion of the data that is not used during training. It is reserved to evaluate the model's performance on unseen data.\n",
        "\n",
        "###Purpose:\n",
        "To assess how well the model generalizes to new data.\n",
        "\n",
        "To ensure that the model is not overfitting or underfitting.\n",
        "\n",
        "###Example:\n",
        "Using the same house price dataset, the testing set would contain a different set of houses, where the model predicts prices based on features. These predictions are compared with the actual prices to evaluate accuracy.\n",
        "\n",
        "###Why Split the Dataset?\n",
        "The main reason for splitting the dataset is to simulate how the model performs on real-world, unseen data. This prevents:\n",
        "\n",
        "1.Overfitting: Where the model memorizes the training data but fails to generalize.\n",
        "\n",
        "2.Data Leakage: When the model has access to the test data during training, leading to overly optimistic performance metrics.\n",
        "\n",
        "###Typical Splits\n",
        "1.Common Ratios:\n",
        "\n",
        "70% training, 30% testing.\n",
        "\n",
        "80% training, 20% testing.\n",
        "\n",
        "For large datasets, 90% training, 10% testing may suffice.\n",
        "\n",
        "2.Validation Dataset:\n",
        "\n",
        "In addition to training and testing sets, a validation set may be used for hyperparameter tuning and model selection.\n",
        "\n",
        "###How It Works\n",
        "1.Training Phase:\n",
        "The model learns patterns by minimizing the error on the training set.\n",
        "\n",
        "2.Testing Phase:\n",
        "After training, the model is evaluated on the testing set.\n",
        "\n",
        "Metrics like accuracy, precision, recall, or mean squared error are computed to measure performance.\n",
        "\n",
        "###Key Metrics for Testing:\n",
        "\n",
        "1.Classification Models:\n",
        "Accuracy, Precision, Recall, F1-Score, ROC-AUC.\n",
        "\n",
        "2.Regression Models:\n",
        "Mean Squared Error (MSE), Mean Absolute Error (MAE),\n",
        "ùëÖ\n",
        "2\n",
        "  Score.\n",
        "\n",
        "In summary, training and testing datasets play crucial roles in building robust machine learning models that perform well on real-world data."
      ],
      "metadata": {
        "id": "kxgoe9bKQC0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8.What is sklearn.preprocessing?\n",
        "\n",
        "sklearn.preprocessing is a module in the scikit-learn library in Python that provides tools and functions for data preprocessing. Preprocessing is a crucial step in machine learning to prepare raw data for modeling. It involves scaling, encoding, normalizing, and transforming data to make it suitable for machine learning algorithms.\n",
        "\n",
        "###Key Features of sklearn.preprocessing\n",
        "\n",
        "1.Data Scaling and Normalization:\n",
        "\n",
        "Adjusts data to ensure that features have a consistent scale or distribution, which is essential for algorithms sensitive to feature magnitudes.\n",
        "\n",
        "2.Encoding Categorical Variables:\n",
        "\n",
        "Transforms non-numeric (categorical) data into numerical formats for machine learning models.\n",
        "\n",
        "3.Imputation and Transformation:\n",
        "\n",
        "Handles missing values and applies mathematical transformations to features.\n",
        "\n",
        "4.Feature Selection:\n",
        "\n",
        "Prepares specific features based on user-defined transformations.\n",
        "\n",
        "###Commonly Used Classes and Functions\n",
        "1.Scaling\n",
        "\n",
        "StandardScaler:\n",
        "\n",
        "Scales data to have a mean of 0 and a standard deviation of 1 (z-score normalization).\n",
        "Useful for algorithms like SVM, logistic regression, and neural networks.\n",
        "```\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "```\n",
        "MinMaxScaler:\n",
        "\n",
        "Scales data to a fixed range, typically\n",
        "[\n",
        "0\n",
        ",\n",
        "1\n",
        "].\n",
        "Preserves the shape of the distribution.\n",
        "```\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "```\n",
        "MaxAbsScaler:\n",
        "\n",
        "Scales data to\n",
        "[\n",
        "‚àí\n",
        "1\n",
        ",\n",
        "1\n",
        "]based on the maximum absolute value of each feature.\n",
        "```\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "scaler = MaxAbsScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "```\n",
        "\n",
        "2.Normalization\n",
        "\n",
        "Normalizes feature vectors to have a unit norm (useful for text classification or clustering).\n",
        "```\n",
        "from sklearn.preprocessing import Normalizer\n",
        "normalizer = Normalizer()\n",
        "normalized_data = normalizer.fit_transform(data)\n",
        "```\n",
        "3. Encoding\n",
        "\n",
        "LabelEncoder:\n",
        "\n",
        "Encodes categorical labels into integers.\n",
        "```\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "encoded_labels = encoder.fit_transform(labels)\n",
        "```\n",
        "OneHotEncoder:\n",
        "\n",
        "Converts categorical features into a binary matrix.\n",
        "```\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder()\n",
        "encoded_features = encoder.fit_transform(features).toarray()\n",
        "```\n",
        "4.Binarization\n",
        "\n",
        "Converts numerical values into binary (0 or 1) based on a threshold.\n",
        "```\n",
        "from sklearn.preprocessing import Binarizer\n",
        "binarizer = Binarizer(threshold=5.0)\n",
        "binary_data = binarizer.fit_transform(data)\n",
        "```\n",
        "\n",
        "5.Polynomial Features\n",
        "\n",
        "Generates new features by computing combinations of existing features.\n",
        "```\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "poly_features = poly.fit_transform(data)\n",
        "```\n",
        "6. Power Transformation\n",
        "\n",
        "Stabilizes variance and makes data more Gaussian-like.\n",
        "\n",
        "PowerTransformer: Applies power transformations like Box-Cox or Yeo-Johnson.\n",
        "```\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "pt = PowerTransformer()\n",
        "transformed_data = pt.fit_transform(data)\n",
        "```\n",
        "\n",
        "###Why Use sklearn.preprocessing?\n",
        "\n",
        "1.Improves Model Performance:\n",
        "Ensures data is on a compatible scale for machine learning algorithms.\n",
        "\n",
        "2.Handles Different Data Types:\n",
        "Encodes categorical and continuous data for compatibility with numerical models.\n",
        "\n",
        "3.Prepares Data for Robustness:\n",
        "Mitigates issues like outliers, skewness, or inconsistent feature magnitudes.\n",
        "\n",
        "Using sklearn.preprocessing, you can seamlessly preprocess your data, ensuring it is clean and ready for model training and evaluation."
      ],
      "metadata": {
        "id": "vcvaNvTBQC32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9.What is a Test set?\n",
        "\n",
        "###Test Set\n",
        "A test set is a portion of the dataset used to evaluate the performance of a trained machine learning model. Unlike the training set, which is used to fit the model, the test set is reserved and never seen by the model during training. It simulates how the model will perform on new, unseen data in real-world scenarios.\n",
        "\n",
        "###Key Characteristics of a Test Set\n",
        "\n",
        "1.Unseen Data:\n",
        "\n",
        "The model does not use this data during training to prevent bias.\n",
        "It helps evaluate the model's generalization ability.\n",
        "\n",
        "2.Separate from Training Data:\n",
        "\n",
        "Typically, the dataset is split into training and test sets, often with ratios like 80-20 or 70-30.\n",
        "\n",
        "3.Performance Metrics:\n",
        "\n",
        "The test set is used to calculate performance metrics such as accuracy, precision, recall, F1-score (for classification), or mean squared error, mean absolute error (for regression).\n",
        "\n",
        "###Why Use a Test Set?\n",
        "\n",
        "1.Evaluates Generalization:\n",
        "\n",
        "Ensures the model performs well on data it hasn‚Äôt seen before.\n",
        "\n",
        "2.Avoids Overfitting:\n",
        "\n",
        "Helps detect if the model has memorized the training data instead of learning general patterns.\n",
        "\n",
        "3.Real-World Simulation:\n",
        "\n",
        "Mimics how the model will behave when deployed in real-world scenarios with new data.\n",
        "\n",
        "###Workflow Involving a Test Set\n",
        "\n",
        "1.Dataset Preparation:\n",
        "\n",
        "Divide the dataset into training and test sets (e.g., 80% training, 20% testing).\n",
        "Optionally, further divide the training set into a validation set for hyperparameter tuning.\n",
        "\n",
        "2.Training Phase:\n",
        "\n",
        "Train the model using the training set.\n",
        "\n",
        "3.Testing Phase:\n",
        "\n",
        "Evaluate the trained model on the test set.\n",
        "\n",
        "4.Performance Metrics:\n",
        "\n",
        "Compute relevant metrics like accuracy, precision, recall, F1-score, or mean squared error.\n",
        "\n",
        "###Example of Test Set Usage in Python\n",
        "```\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Example dataset\n",
        "X = [[1, 2], [2, 3], [3, 4], [4, 5]]  # Features\n",
        "y = [0, 1, 0, 1]  # Labels\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "```\n",
        "\n",
        "###Good Practices\n",
        "\n",
        "1.Do Not Peek:\n",
        "Avoid using the test set for hyperparameter tuning or feature engineering; reserve it strictly for final evaluation.\n",
        "\n",
        "2.Balanced Test Set:\n",
        "Ensure the test set represents the overall dataset distribution (e.g., similar class proportions in classification tasks).\n",
        "\n",
        "3.Cross-Validation:\n",
        "When data is limited, use techniques like k-fold cross-validation to validate the model multiple times and estimate performance more robustly.\n",
        "\n",
        "The test set provides a final, unbiased measure of how well your machine learning model is likely to perform in production."
      ],
      "metadata": {
        "id": "AWj6uceYQC6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10.How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
        "\n",
        "In machine learning, the data is typically split into training and test sets to evaluate the model's ability to generalize to unseen data. The training set is used to train the model, while the test set is used to evaluate its performance.\n",
        "\n",
        "Here's a step-by-step guide to splitting data in Python using scikit-learn:\n",
        "\n",
        "1. Import Necessary Libraries\n",
        "python\n",
        "Copy code\n",
        "from sklearn.model_selection import train_test_split\n",
        "2. Prepare Your Data\n",
        "Assume you have a dataset with features (X) and target labels (y):\n",
        "\n",
        "```\n",
        "X = [[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7]]  # Example features\n",
        "y = [0, 1, 0, 1, 0, 1]  # Example labels\n",
        "```\n",
        "3. Split the Data\n",
        "Use train_test_split to split the dataset into training and testing sets:\n",
        "\n",
        "```\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "```\n",
        "Here:\n",
        "\n",
        "test_size=0.2: 20% of the data will be used for testing, and the remaining 80% will be used for training.\n",
        "random_state=42: Ensures that the split is reproducible (i.e., you get the same split every time you run the code).\n",
        "4. Confirm the Split\n",
        "You can print the sizes of the splits to confirm:\n",
        "\n",
        "```\n",
        "print(\"Training features shape:\", len(X_train))\n",
        "print(\"Testing features shape:\", len(X_test))\n",
        "```\n",
        "\n",
        "###How Do You Approach a Machine Learning Problem?\n",
        "\n",
        "Approaching a machine learning problem involves a series of steps from understanding the problem to evaluating and deploying the model. Here's a typical approach:\n",
        "\n",
        "1. Define the Problem\n",
        "\n",
        "Problem Type: Is it a classification (e.g., predicting whether a customer will churn) or regression problem (e.g., predicting house prices)?\n",
        "\n",
        "Business Goal: What are you trying to optimize? This helps define evaluation metrics.\n",
        "\n",
        "2. Collect and Prepare the Data\n",
        "\n",
        "Gather Data: Obtain data from various sources, such as databases, CSV files, APIs, or scraping.\n",
        "\n",
        "Data Exploration: Use techniques like EDA (Exploratory Data Analysis) to understand the structure, patterns, and relationships in the data.\n",
        "\n",
        "Common tasks during EDA:\n",
        "\n",
        "Check for missing values: Use df.isnull().sum().\n",
        "Understand the distribution: Use histograms and box plots.\n",
        "\n",
        "Check correlations: Use scatter plots or a correlation matrix.\n",
        "\n",
        "Data Cleaning: Handle missing data, remove duplicates, and deal with outliers.\n",
        "\n",
        "3. Data Preprocessing\n",
        "\n",
        "Handle Categorical Variables: Use encoding techniques like One-Hot Encoding or Label Encoding for categorical features.\n",
        "\n",
        "Feature Scaling: Apply StandardScaler, MinMaxScaler, or other techniques to scale numerical features to a similar range.\n",
        "\n",
        "Feature Engineering: Create new features based on existing ones (e.g., extracting date components, polynomial features).\n",
        "\n",
        "4. Split the Data\n",
        "\n",
        "Training and Testing Split: Use train_test_split to create training and testing sets.\n",
        "\n",
        "Optionally, use cross-validation for better model evaluation.\n",
        "\n",
        "5. Choose a Model\n",
        "\n",
        "Select a model that fits the problem type:\n",
        "\n",
        "Classification: Logistic Regression, Decision Trees, Random Forest, Support Vector Machine (SVM), k-NN, etc.\n",
        "\n",
        "Regression: Linear Regression, Decision Trees, Random Forest, k-NN, etc.\n",
        "\n",
        "Clustering: k-Means, DBSCAN, Hierarchical Clustering, etc.\n",
        "\n",
        "6. Train the Model\n",
        "\n",
        "Train the selected model on the training set using model.fit(X_train, y_train).\n",
        "```\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "7. Evaluate the Model\n",
        "\n",
        "Testing: Evaluate the model on the test set using performance metrics like accuracy, precision, recall, F1-score, and ROC-AUC for classification tasks, or Mean Squared Error (MSE) for regression tasks.\n",
        "\n",
        "```\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "```\n",
        "8. Hyperparameter Tuning\n",
        "\n",
        "Grid Search or Random Search: Find the best hyperparameters using GridSearchCV or RandomizedSearchCV.\n",
        "Example for tuning hyperparameters:\n",
        "```\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'C': [0.1, 1, 10], 'solver': ['liblinear', 'saga']}\n",
        "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "```\n",
        "\n",
        "9. Model Interpretation\n",
        "\n",
        "Understand the model's decision-making process, especially for complex models like Random Forest or neural networks.\n",
        "\n",
        "Use techniques like SHAP (SHapley Additive exPlanations) or LIME for model explainability.\n",
        "\n",
        "10. Model Deployment\n",
        "\n",
        "Final Model: Once the model is trained and tuned, deploy it into production for real-time predictions or batch processing.\n",
        "\n",
        "Monitoring: Monitor the model's performance over time to detect any decline in accuracy due to changing data (data drift).\n",
        "\n",
        "###Example Workflow:\n",
        "Here‚Äôs a typical example workflow in Python:\n",
        "\n",
        "```\n",
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 2: Load and prepare the data\n",
        "df = pd.read_csv('data.csv')\n",
        "X = df.drop('target', axis=1)  # Features\n",
        "y = df['target']  # Target variable\n",
        "\n",
        "# Step 3: Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Choose a model and train it\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions and evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "```\n",
        "\n",
        "Summary:\n",
        "Define the problem and understand the data.\n",
        "Preprocess the data by handling missing values, encoding, scaling, etc.\n",
        "\n",
        "Split the data into training and testing sets.\n",
        "Choose the appropriate model, train it, and evaluate it.\n",
        "\n",
        "Tune the model to improve performance and deploy it in real-world applications.\n",
        "\n",
        "This structured approach ensures that you build a model that is both accurate and generalizes well to new data."
      ],
      "metadata": {
        "id": "fxoqtknzQC9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#11.Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "Exploratory Data Analysis (EDA) is a crucial step before fitting a machine learning model because it allows you to better understand the data, identify potential issues, and ensure that the data is in the best possible form for modeling. Here are the key reasons why EDA is performed before model fitting:\n",
        "\n",
        "1. Understanding the Data\n",
        "\n",
        "Gain Insights: EDA helps you understand the characteristics of the data, including the types of features (numerical or categorical), distribution, and patterns. This helps in selecting the right model and understanding which features are most relevant to the problem.\n",
        "\n",
        "Identify Key Variables: It allows you to identify which features are important and which ones might be irrelevant or redundant, helping in feature selection.\n",
        "\n",
        "2. Detecting Data Issues\n",
        "\n",
        "Missing Values: EDA can help identify missing values in the dataset. Missing data can affect the model's performance, and understanding the missing data pattern helps you decide how to handle it (e.g., imputation or removal).\n",
        "\n",
        "Outliers: Identifying outliers is essential because they can distort model training, especially for models like linear regression. EDA helps in recognizing whether outliers need to be handled (e.g., via transformations or removal).\n",
        "\n",
        "Incorrect Data Types: During EDA, you may discover that some features are incorrectly formatted (e.g., numerical data stored as strings), and these need to be corrected before fitting the model.\n",
        "\n",
        "Duplicate Records: Sometimes, datasets may have duplicate rows that can lead to biased model training. EDA helps in identifying and removing these duplicates.\n",
        "\n",
        "3. Data Distribution and Scaling\n",
        "\n",
        "Check Distributions: EDA provides insights into the distributions of numerical features, helping you understand if any transformations (like scaling or normalization) are needed. For instance, some models like Logistic Regression or Support Vector Machines (SVM) are sensitive to the scale of the data.\n",
        "\n",
        "Skewed Data: Some machine learning algorithms, such as linear models, may not perform well if the data is heavily skewed. EDA can reveal whether the data requires transformations (e.g., log transformation) to achieve a normal distribution.\n",
        "\n",
        "4. Identifying Correlations\n",
        "\n",
        "Feature Relationships: EDA helps you identify correlations between features. Understanding these relationships can guide you in feature selection (removing highly correlated features to avoid multicollinearity) and engineering new features. It also helps in understanding the interactions between different variables.\n",
        "\n",
        "Target-Feature Relationship: By visualizing the relationship between features and the target variable, you can determine which features have a strong predictive power, which is important for choosing a model and tuning its parameters.\n",
        "\n",
        "5. Choosing the Right Model\n",
        "\n",
        "Model Suitability: Based on the patterns discovered in the data, EDA can help you determine the type of machine learning model that may be most appropriate. For example:\n",
        "If the target variable is categorical, you might choose a classification model.\n",
        "If the target is continuous, you might go for regression models.\n",
        "If there are temporal patterns (time-series data), a time-series model might be needed.\n",
        "\n",
        "Imbalanced Classes: If the dataset has imbalanced classes (in classification tasks), you can detect this during EDA and take steps to handle it (e.g., using SMOTE, class weighting, or sampling techniques).\n",
        "\n",
        "6. Feature Engineering\n",
        "\n",
        "Creating New Features: EDA can help generate new features from existing ones. For example, by examining temporal data, you might create features such as \"day of the week\" or \"hour of the day.\"\n",
        "\n",
        "Feature Selection: You can identify irrelevant features or redundant ones, allowing you to select the most important features for modeling.\n",
        "\n",
        "7. Improving Model Accuracy\n",
        "\n",
        "Refining the Dataset: EDA can guide the process of cleaning, transforming, and preparing the dataset, which often leads to better model performance. Understanding the data thoroughly before fitting a model helps in avoiding pitfalls that could reduce accuracy.\n",
        "\n",
        "###Example: Steps in EDA for Model Preparation\n",
        "1.Load the Data: Import the dataset.\n",
        "\n",
        "2.Summary Statistics: Check the summary of the dataset (e.g., mean, median, mode, standard deviation).\n",
        "\n",
        "3.Missing Data: Identify columns with missing data and decide how to handle it.\n",
        "\n",
        "4.Visualize Distributions: Plot histograms, box plots, and scatter plots to understand feature distributions and relationships.\n",
        "\n",
        "5.Correlation Analysis: Use correlation matrices to identify highly correlated features.\n",
        "\n",
        "6.Outliers: Visualize data points to detect any outliers and decide how to address them.\n",
        "\n",
        "7.Target Distribution: Check the distribution of the target variable, especially in classification problems (imbalanced classes).\n",
        "\n",
        "8.Feature Engineering: Create or transform features based on insights from EDA.\n",
        "\n",
        "###Conclusion\n",
        "\n",
        "Performing EDA before fitting a machine learning model is essential because it ensures that the data is clean, structured, and well-understood. It helps uncover underlying patterns, potential issues (like missing data or outliers), and relationships between features and the target variable. This preparatory step significantly improves the model-building process, resulting in more accurate and reliable predictions."
      ],
      "metadata": {
        "id": "WFRo0mu_QDCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#12.What is correlation?\n",
        "\n",
        "Correlation is a statistical measure that describes the strength and direction of a relationship between two variables. It helps in understanding whether and how strongly variables are related.\n",
        "\n",
        "###Positive Correlation:\n",
        "When one variable increases, the other variable also increases.\n",
        "\n",
        "###Negative Correlation:\n",
        "When one variable increases, the other variable decreases.\n",
        "\n",
        "###No Correlation:\n",
        "When there is no consistent relationship between the variables.\n",
        "\n",
        "The correlation is typically represented by the correlation coefficient ( ùëü), which ranges from ‚àí 1 to 1 :\n",
        "\n",
        "ùëü = 1: Perfect positive correlation.\n",
        "\n",
        "ùëü = ‚àí 1: Perfect negative correlation.\n",
        "\n",
        "ùëü = 0: No correlation."
      ],
      "metadata": {
        "id": "Ux2OWDPmQDFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#13.What does negative correlation mean?\n",
        "\n",
        "###Negative Correlation:\n",
        "\n",
        "Negative correlation indicates an inverse relationship between two variables. As one variable increases, the other decreases, and vice versa.\n",
        "\n",
        "###Examples:\n",
        "\n",
        "Real-world Example: Hours spent watching TV and exam scores. As hours spent watching TV increase, exam scores might decrease.\n",
        "\n",
        "Financial Example: Price of a product and its demand. As the price of a product increases, its demand typically decreases.\n",
        "\n",
        "Visual Representation: In a scatter plot, a negative correlation shows points trending downward from left to right.\n",
        "\n",
        "###Interpreting the Strength of Correlation:\n",
        "\n",
        "‚àí 1 ‚â§ ùëü < ‚àí 0.7: Strong negative correlation.\n",
        "\n",
        "‚àí 0.7 ‚â§ ùëü < ‚àí 0.3: Moderate negative correlation.\n",
        "\n",
        "‚àí 0.3 ‚â§ ùëü < 0: Weak negative correlation.\n",
        "\n",
        "Understanding correlation helps in exploring relationships between variables, which is crucial for predictive modeling and data analysis."
      ],
      "metadata": {
        "id": "LGawTsYifnxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#14.How can you find correlation between variables in Python?\n",
        "\n",
        "To find the correlation between variables in Python, you can use the Pandas library, which provides an easy-to-use method for computing correlations between numerical variables. The most common correlation measure is Pearson's correlation coefficient, but Pandas also supports other methods like Spearman and Kendall.\n",
        "\n",
        "Here‚Äôs how you can calculate correlation between variables in Python:\n",
        "\n",
        "1. Using Pandas corr() method\n",
        "\n",
        "The corr() method computes the pairwise correlation of columns in a DataFrame, excluding missing values.\n",
        "\n",
        "Step-by-Step Example\n",
        "```\n",
        "import pandas as pd\n",
        "\n",
        "# Example DataFrame\n",
        "data = {\n",
        "    'A': [1, 2, 3, 4, 5],\n",
        "    'B': [5, 4, 3, 2, 1],\n",
        "    'C': [2, 3, 4, 5, 6]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Display the correlation matrix\n",
        "print(correlation_matrix)\n",
        "```\n",
        "Output:\n",
        "```\n",
        "          A    B    C\n",
        "A  1.000000 -1.000000  1.000000\n",
        "B -1.000000  1.000000 -1.000000\n",
        "C  1.000000 -1.000000  1.000000\n",
        "```\n",
        "\n",
        "The correlation matrix shows the pairwise correlations between the columns.\n",
        "\n",
        "The diagonal values are always 1 because a variable is always perfectly correlated with itself.\n",
        "\n",
        "The values range from -1 to 1:\n",
        "\n",
        "1: Perfect positive correlation.\n",
        "\n",
        "-1: Perfect negative correlation.\n",
        "\n",
        "0: No correlation.\n",
        "\n",
        "Correlations in this example:\n",
        "\n",
        "A and B have a perfect negative correlation (-1).\n",
        "\n",
        "A and C have a perfect positive correlation (1).\n",
        "\n",
        "B and C also have a perfect negative correlation (-1).\n",
        "\n",
        "2. Using Seaborn for Visualization\n",
        "\n",
        "You can also use Seaborn to visualize the correlation matrix using a heatmap, which makes it easier to interpret correlations visually.\n",
        "\n",
        "```\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Create a heatmap\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Output:\n",
        "A heatmap will be displayed showing the correlation values between different pairs of features.\n",
        "\n",
        "3. Using Different Correlation Methods\n",
        "\n",
        "By default, Pandas corr() uses Pearson's correlation, but you can also use other methods such as Spearman or Kendall.\n",
        "\n",
        "Spearman: Measures monotonic relationships (not necessarily linear).\n",
        "\n",
        "Kendall: A method for rank correlation.\n",
        "Example with Spearman and Kendall correlation:\n",
        "```\n",
        "# Calculate Spearman correlation\n",
        "spearman_corr = df.corr(method='spearman')\n",
        "print(\"Spearman Correlation:\\n\", spearman_corr)\n",
        "\n",
        "# Calculate Kendall correlation\n",
        "kendall_corr = df.corr(method='kendall')\n",
        "print(\"Kendall Correlation:\\n\", kendall_corr)\n",
        "```\n",
        "\n",
        "4. Correlation with Specific Variables\n",
        "\n",
        "If you want to find the correlation between a specific feature and the target variable (or any other variable), you can select the columns of interest from the correlation matrix:\n",
        "\n",
        "```\n",
        "# Correlation between 'A' and other variables\n",
        "correlation_A = df['A'].corr(df['B'])  # A and B\n",
        "print(\"Correlation between A and B:\", correlation_A)\n",
        "\n",
        "# Correlation between 'A' and 'C'\n",
        "correlation_A_C = df['A'].corr(df['C'])\n",
        "print(\"Correlation between A and C:\", correlation_A_C)\n",
        "```\n",
        "\n",
        "5. Handling Missing Data\n",
        "\n",
        "Correlation methods typically exclude missing values by default. However, if your dataset has missing values and you want to handle them explicitly, you can use:\n",
        "\n",
        "```\n",
        "# Fill missing values with mean or any strategy you prefer\n",
        "df_filled = df.fillna(df.mean())\n",
        "\n",
        "# Calculate correlation on the filled data\n",
        "correlation_matrix_filled = df_filled.corr()\n",
        "print(correlation_matrix_filled)\n",
        "```\n",
        "\n",
        "Or, you can drop rows with missing values before computing correlations:\n",
        "\n",
        "```\n",
        "# Drop rows with missing values\n",
        "df_dropped = df.dropna()\n",
        "\n",
        "# Calculate correlation on the dropped data\n",
        "correlation_matrix_dropped = df_dropped.corr()\n",
        "print(correlation_matrix_dropped)\n",
        "```\n",
        "\n",
        "###Conclusion\n",
        "\n",
        "The corr() method in Pandas is a simple and effective way to calculate correlation between numerical variables in a dataset.\n",
        "Seaborn can be used to visualize correlations in the form of a heatmap, which makes it easier to interpret relationships.\n",
        "\n",
        "You can choose different correlation methods (e.g., Spearman or Kendall) depending on the type of relationship you're interested in."
      ],
      "metadata": {
        "id": "KYK90Z8dQDIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example DataFrame\n",
        "data = {\n",
        "    'A': [1, 2, 3, 4, 5],\n",
        "    'B': [5, 4, 3, 2, 1],\n",
        "    'C': [2, 3, 4, 5, 6]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Display the correlation matrix\n",
        "print(correlation_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjoggflIhfHe",
        "outputId": "ebe5bf1c-0c5d-4292-fd08-3d987a8c0ef5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     A    B    C\n",
            "A  1.0 -1.0  1.0\n",
            "B -1.0  1.0 -1.0\n",
            "C  1.0 -1.0  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Create a heatmap\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "x1JL907AhgB3",
        "outputId": "4e23c557-bbf1-4849-b590-dfba8d061d0b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Calculate Spearman correlation\n",
        "spearman_corr = df.corr(method='spearman')\n",
        "print(\"Spearman Correlation:\\n\", spearman_corr)\n",
        "\n",
        "# Calculate Kendall correlation\n",
        "kendall_corr = df.corr(method='kendall')\n",
        "print(\"Kendall Correlation:\\n\", kendall_corr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXD2r71thn3l",
        "outputId": "9fbf8348-89c4-4e68-9753-21b9f4dca6f0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spearman Correlation:\n",
            "      A    B    C\n",
            "A  1.0 -1.0  1.0\n",
            "B -1.0  1.0 -1.0\n",
            "C  1.0 -1.0  1.0\n",
            "Kendall Correlation:\n",
            "      A    B    C\n",
            "A  1.0 -1.0  1.0\n",
            "B -1.0  1.0 -1.0\n",
            "C  1.0 -1.0  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation between 'A' and other variables\n",
        "correlation_A = df['A'].corr(df['B'])  # A and B\n",
        "print(\"Correlation between A and B:\", correlation_A)\n",
        "\n",
        "# Correlation between 'A' and 'C'\n",
        "correlation_A_C = df['A'].corr(df['C'])\n",
        "print(\"Correlation between A and C:\", correlation_A_C)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjyaTcmUhpaJ",
        "outputId": "0a3cbb57-d155-4d9c-a8df-cc8c2c030249"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation between A and B: -0.9999999999999999\n",
            "Correlation between A and C: 0.9999999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values with mean or any strategy you prefer\n",
        "df_filled = df.fillna(df.mean())\n",
        "\n",
        "# Calculate correlation on the filled data\n",
        "correlation_matrix_filled = df_filled.corr()\n",
        "print(correlation_matrix_filled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGLO5HYQhpY2",
        "outputId": "3e9ac41f-34ea-438b-daf6-2668b9d04789"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     A    B    C\n",
            "A  1.0 -1.0  1.0\n",
            "B -1.0  1.0 -1.0\n",
            "C  1.0 -1.0  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing values\n",
        "df_dropped = df.dropna()\n",
        "\n",
        "# Calculate correlation on the dropped data\n",
        "correlation_matrix_dropped = df_dropped.corr()\n",
        "print(correlation_matrix_dropped)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BK3n0GgChpVO",
        "outputId": "2571aa2e-d90f-4c9f-8544-7bb253b80834"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     A    B    C\n",
            "A  1.0 -1.0  1.0\n",
            "B -1.0  1.0 -1.0\n",
            "C  1.0 -1.0  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#15.What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "Causation refers to a cause-and-effect relationship between two variables, where a change in one variable directly leads to a change in another. In other words, causation means that one variable actively influences the other, and there is a direct cause for the observed effect.\n",
        "\n",
        "For example, if you increase the temperature in a room, it directly causes the room's air pressure to increase (assuming all other factors are constant). Here, the change in temperature causes the change in air pressure.\n",
        "\n",
        "###Difference Between Correlation and Causation\n",
        "\n",
        "The key difference between correlation and causation is that correlation simply indicates a relationship between two variables, while causation goes a step further to show that one variable directly causes the change in another.\n",
        "\n",
        "1. Correlation\n",
        "\n",
        "Correlation indicates that two variables move together in some way (either positively or negatively). However, correlation alone does not imply that one variable causes the other to change.\n",
        "\n",
        "Correlation can be positive (both variables increase together) or negative (one variable increases while the other decreases).\n",
        "\n",
        "Correlation only measures the strength and direction of the relationship between variables, but it does not provide evidence of a direct cause-and-effect relationship.\n",
        "\n",
        "2. Causation\n",
        "\n",
        "Causation, on the other hand, shows that one variable has a direct effect on another. It implies that a change in one variable directly causes a change in another, often supported by experimental or empirical evidence.\n",
        "\n",
        "To establish causation, you need to conduct controlled experiments or use statistical techniques like randomized controlled trials (RCTs), where variables are manipulated to observe their effects.\n",
        "\n",
        "###Example: Correlation vs. Causation\n",
        "Let's consider an example:\n",
        "\n",
        "###Scenario: Ice Cream Sales and Drowning Incidents\n",
        "\n",
        "Observation: There is a positive correlation between ice cream sales and drowning incidents. As ice cream sales increase, so do the number of drowning incidents.\n",
        "\n",
        "###Correlation:\n",
        "\n",
        "The data shows a positive correlation between ice cream sales and drowning incidents.\n",
        "Correlation coefficient might be high, indicating that both variables tend to increase together (e.g., in summer months).\n",
        "\n",
        "###Causation?:\n",
        "No, the increase in ice cream sales does not cause drowning incidents.\n",
        "\n",
        "The actual cause is likely to be warmer weather (summer), which increases both the number of people swimming (leading to more drowning incidents) and the desire to eat ice cream.\n",
        "\n",
        "Thus, weather is the common cause, not a direct causal relationship between ice cream sales and drownings.\n",
        "\n",
        "###Key Point:\n",
        "\n",
        "The observed correlation does not imply that eating ice cream causes drowning.\n",
        "\n",
        "This is a classic example of the \"lurking variable\" or \"confounding variable\", where an external factor (in this case, the weather) influences both variables.\n",
        "\n",
        "###Real-World Example of Causation: Smoking and Lung Cancer\n",
        "\n",
        "Observation: Studies have shown that smoking is causally linked to lung cancer.\n",
        "\n",
        "###Causation:\n",
        "\n",
        "There is strong empirical evidence and biological mechanisms that explain how smoking directly causes lung cancer.\n",
        "\n",
        "The toxic chemicals in cigarette smoke damage the lungs over time, leading to the development of cancer. This is a cause-and-effect relationship.\n",
        "\n",
        "This relationship has been confirmed through many scientific studies, including controlled experiments and longitudinal studies, where smokers are observed over long periods and their health outcomes are studied.\n",
        "\n",
        "###Conclusion\n",
        "\n",
        "Correlation simply indicates that two variables are related in some way, but it doesn‚Äôt tell us anything about whether one causes the other.\n",
        "\n",
        "Causation means that one variable directly influences another. Establishing causation requires rigorous scientific methods, such as experiments or long-term studies, to demonstrate that one event causes another.\n",
        "\n",
        "Understanding the difference between correlation and causation is crucial in data analysis and helps avoid the common pitfall of assuming that correlated variables are causally linked."
      ],
      "metadata": {
        "id": "DABx3c44iAi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#16.What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "In machine learning and deep learning, an optimizer is an algorithm or method used to adjust the weights and biases of a model during training to minimize the loss function. The loss function measures the difference between the predicted output and the actual target value. The goal of the optimizer is to find the set of parameters (weights and biases) that results in the lowest possible loss.\n",
        "\n",
        "Optimizers work by calculating gradients (partial derivatives) of the loss function with respect to the model parameters and using these gradients to adjust the parameters in a way that reduces the loss. This is often done using a technique called gradient descent.\n",
        "\n",
        "###Types of Optimizers\n",
        "There are several types of optimizers, and each one has its own strengths and is suitable for different types of models and data. Below are the main types of optimizers:\n",
        "\n",
        "###1. Gradient Descent (GD)\n",
        "\n",
        "Gradient Descent is the most basic optimization algorithm. It works by computing the gradient (or derivative) of the loss function with respect to each parameter and adjusting the parameters in the direction that reduces the loss.\n",
        "\n",
        "###Steps:\n",
        "\n",
        "Compute the gradient of the loss function with respect to the weights.\n",
        "Update the weights in the opposite direction of the gradient by a small step size (learning rate).\n",
        "Formula:\n",
        "ùúÉ\n",
        "=\n",
        "ùúÉ\n",
        "‚àí\n",
        "ùúÇ\n",
        "‚àá\n",
        "ùúÉ\n",
        "ùêΩ\n",
        "(\n",
        "ùúÉ\n",
        ")\n",
        "\n",
        "Where:\n",
        "\n",
        "ùúÉ\n",
        " represents the model parameters (weights and biases),\n",
        "\n",
        "ùúÇis the learning rate,\n",
        "\n",
        "‚àá\n",
        "ùúÉ\n",
        "ùêΩ\n",
        "(\n",
        "ùúÉ\n",
        ") is the gradient of the loss function with respect to\n",
        "ùúÉ\n",
        "\n",
        ".\n",
        "###Example:\n",
        "Consider training a linear regression model. If the loss function is Mean Squared Error (MSE), the gradient descent algorithm would update the weights to minimize the MSE.\n",
        "\n",
        "###Pros:\n",
        "Simple and widely used.\n",
        "\n",
        "Works well when the dataset is not too large.\n",
        "\n",
        "###Cons:\n",
        "Can be slow for large datasets.\n",
        "\n",
        "It can get stuck in local minima for non-convex loss functions.\n",
        "\n",
        "###2. Stochastic Gradient Descent (SGD)\n",
        "Stochastic Gradient Descent is a variation of gradient descent where the parameters are updated using only one or a few training examples (data points) at a time instead of the whole dataset. This makes it faster but introduces more noise into the parameter updates.\n",
        "\n",
        "###Steps:\n",
        "Pick a random data point from the training set.\n",
        "Compute the gradient for that data point and update the model parameters accordingly.\n",
        "Formula:\n",
        "ùúÉ\n",
        "=\n",
        "ùúÉ\n",
        "‚àí\n",
        "ùúÇ\n",
        "‚àá\n",
        "ùúÉ\n",
        "ùêΩ\n",
        "(\n",
        "ùúÉ\n",
        ";\n",
        "ùë•\n",
        "ùëñ\n",
        ",\n",
        "ùë¶\n",
        "ùëñ\n",
        ")\n",
        "Where\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        ",\n",
        "ùë¶\n",
        "ùëñ\n",
        ")is a single data point from the training set.\n",
        "\n",
        "###Example:\n",
        "For a deep learning model, if you use SGD, the model weights would be updated after processing each individual sample or mini-batch of data rather than after processing the entire dataset.\n",
        "\n",
        "###Pros:\n",
        "Faster than traditional gradient descent for large datasets.\n",
        "\n",
        "Less memory required because you don‚Äôt need to store the entire dataset.\n",
        "\n",
        "###Cons:\n",
        "The updates can be noisy, and the optimization path may not be as smooth.\n",
        "\n",
        "It can oscillate around the minimum if the learning rate is not properly tuned.\n",
        "\n",
        "###3. Mini-Batch Gradient Descent\n",
        "Mini-Batch Gradient Descent is a compromise between Batch Gradient Descent (where the entire dataset is used) and Stochastic Gradient Descent (where only one data point is used). It updates the parameters after processing a small random subset of the dataset, called a mini-batch.\n",
        "\n",
        "###Steps:\n",
        "Divide the training set into smaller batches (mini-batches).\n",
        "\n",
        "Update the parameters after processing each mini-batch.\n",
        "\n",
        "Formula:\n",
        "ùúÉ\n",
        "=\n",
        "ùúÉ\n",
        "‚àí\n",
        "ùúÇ\n",
        "‚àá\n",
        "ùúÉ\n",
        "ùêΩ\n",
        "(\n",
        "ùúÉ\n",
        ";\n",
        "ùëã\n",
        "mini-batch\n",
        ",\n",
        "ùëå\n",
        "mini-batch\n",
        ")\n",
        "Where\n",
        "ùëã\n",
        "mini-batch\n",
        "  and\n",
        "ùëå\n",
        "mini-batch\n",
        "  are the inputs and outputs of the mini-batch.\n",
        "\n",
        "###Example:\n",
        "In deep learning, training with mini-batches is common. For example, if the training set contains 1000 data points, you might choose a mini-batch size of 100, so after each mini-batch, the weights are updated.\n",
        "\n",
        "###Pros:\n",
        "Faster than batch gradient descent.\n",
        "Reduces the variance of the parameter updates, leading to more stable convergence.\n",
        "\n",
        "###Cons:\n",
        "The choice of mini-batch size can affect performance.\n",
        "\n",
        "Can still be slower than SGD in some cases.\n",
        "\n",
        "###4. Momentum\n",
        "Momentum helps accelerate gradient descent by considering the past gradients to smooth out the updates. Instead of updating the weights using just the current gradient, momentum uses a weighted average of past gradients.\n",
        "\n",
        "\n",
        "Formula:\n",
        "ùë£\n",
        "ùë°\n",
        "=\n",
        "ùõΩ\n",
        "ùë£\n",
        "ùë°\n",
        "‚àí\n",
        "1\n",
        "+\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùõΩ\n",
        ")\n",
        "‚àá\n",
        "ùúÉ\n",
        "ùêΩ\n",
        "(\n",
        "ùúÉ\n",
        ")\n",
        "v\n",
        "t\n",
        "\n",
        "Where:\n",
        "\n",
        "ùë£\n",
        "ùë°\n",
        "\n",
        "is the velocity (accumulated gradient),\n",
        "\n",
        "ùõΩis the momentum factor (usually between 0 and 1),\n",
        "\n",
        "ùúÇ is the learning rate.\n",
        "\n",
        "###Example:\n",
        "In training a neural network, momentum would help the optimizer move more smoothly in the direction of the global minimum by smoothing out the updates over time.\n",
        "\n",
        "###Pros:\n",
        "Helps in faster convergence by overcoming local minima.\n",
        "\n",
        "Reduces oscillations and smooths the path to the minimum.\n",
        "\n",
        "###Cons:\n",
        "Requires careful tuning of the momentum parameter\n",
        "ùõΩ.\n",
        "\n",
        "###5. Adam (Adaptive Moment Estimation)\n",
        "Adam is one of the most popular and advanced optimizers in deep learning. It combines the advantages of both AdaGrad and Momentum. Adam computes adaptive learning rates for each parameter by considering both the first-order momentum and the second-order acceleration of gradients.\n",
        "\n",
        "###Steps:\n",
        "Compute running averages of both the gradient and the squared gradient.\n",
        "Adjust the learning rate for each parameter using these running averages.\n",
        "\n",
        "###Example:\n",
        "\n",
        "In deep learning tasks, Adam is widely used for training models like CNNs (Convolutional Neural Networks) or LSTMs (Long Short-Term Memory networks), where it adjusts the learning rate during training for each parameter.\n",
        "\n",
        "###Pros:\n",
        "Requires little tuning and adapts well to the problem.\n",
        "\n",
        "Works well for large datasets and complex models.\n",
        "It‚Äôs very popular and performs well on a wide range of tasks.\n",
        "\n",
        "###Cons:\n",
        "\n",
        "Can be computationally expensive.\n",
        "May not work well with very sparse data.\n",
        "\n",
        "###6. AdaGrad\n",
        "AdaGrad (Adaptive Gradient Algorithm) adjusts the learning rate based on the frequency of updates for each parameter. It gives smaller updates for frequently occurring features and larger updates for infrequent ones.\n",
        "\n",
        "Formula:\n",
        "ùúÉ\n",
        "=\n",
        "ùúÉ\n",
        "‚àí\n",
        "ùúÇ\n",
        "ùê∫\n",
        "ùë°\n",
        "+\n",
        "ùúñ\n",
        "‚àá\n",
        "ùúÉ\n",
        "ùêΩ\n",
        "(\n",
        "ùúÉ\n",
        ")\n",
        "Where:\n",
        "\n",
        "ùê∫\n",
        "ùë°\n",
        "  is the sum of squared gradients up to time step\n",
        "ùë°,\n",
        "ùúñ is a small number to prevent division by zero.\n",
        "\n",
        "###Example:\n",
        "AdaGrad works well in natural language processing (NLP) tasks where the frequency of features (words) can vary widely.\n",
        "\n",
        "###Pros:\n",
        "Automatically adjusts learning rates for each parameter.\n",
        "\n",
        "Effective for sparse data.\n",
        "\n",
        "###Cons:\n",
        "\n",
        "Can lead to excessively small learning rates in the long run, making it difficult to converge.\n",
        "\n",
        "###Conclusion\n",
        "The choice of optimizer can significantly affect the performance and training time of a machine learning model. Here‚Äôs a summary of the most\n",
        "common optimizers:\n",
        "\n",
        "Gradient Descent (GD): Simple but slow for large datasets.\n",
        "\n",
        "Stochastic Gradient Descent (SGD): Faster but noisier.\n",
        "\n",
        "Mini-Batch Gradient Descent: A balance between speed and stability.\n",
        "\n",
        "Momentum: Speeds up convergence and reduces oscillations.\n",
        "\n",
        "Adam: Most commonly used, combining the benefits of momentum and adaptive learning rates.\n",
        "\n",
        "AdaGrad: Adapts the learning rate based on the frequency of features. Best for sparse data.\n",
        "\n",
        "\n",
        "Choosing the right optimizer depends on the nature of the problem, the size of the dataset, and the complexity of the model being trained."
      ],
      "metadata": {
        "id": "k9FSnTMziAee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#17.What is sklearn.linear_model ?\n",
        "\n",
        "The sklearn.linear_model module in Scikit-learn contains a collection of linear models for regression and classification problems. These models are based on linear relationships between the features (independent variables) and the target variable (dependent variable). Linear models aim to fit a linear equation to the data.\n",
        "\n",
        "The module provides implementations of several linear algorithms, ranging from basic linear regression to more advanced techniques like logistic regression and regularization methods (Ridge, Lasso, Elastic Net, etc.).\n",
        "\n",
        "###Key Algorithms in sklearn.linear_model\n",
        "###1. Linear Regression\n",
        "Purpose: Used for regression problems, where the target variable is continuous.\n",
        "Description: Fits a linear equation to the data:\n",
        "ùë¶\n",
        "=\n",
        "ùë§\n",
        "1\n",
        "ùë•\n",
        "1\n",
        "+\n",
        "ùë§\n",
        "2\n",
        "ùë•\n",
        "2\n",
        "+\n",
        "‚ãØ\n",
        "+\n",
        "ùë§\n",
        "ùëõ\n",
        "ùë•\n",
        "ùëõ\n",
        "+\n",
        "b\n",
        "where\n",
        "ùë§\n",
        "1\n",
        ",\n",
        "ùë§\n",
        "2\n",
        ",\n",
        "‚Ä¶\n",
        ",\n",
        "ùë§\n",
        "ùëõ\n",
        "w\n",
        "  are the weights (coefficients), and\n",
        "ùëè is the bias term.\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Create model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "```\n",
        "\n",
        "###2. Logistic Regression\n",
        "\n",
        "Purpose: Used for classification problems, especially binary classification.\n",
        "Description: Estimates probabilities using the logistic (sigmoid) function. It outputs values between 0 and 1, which can be converted into class predictions.\n",
        "\n",
        "###Example:\n",
        "\n",
        "```\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Create model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "```\n",
        "\n",
        "###3. Ridge Regression\n",
        "\n",
        "Purpose: Regression with L2 regularization, used to prevent overfitting by penalizing large coefficients.\n",
        "\n",
        "Description: Modifies the loss function by adding the sum of squared coefficients:\n",
        "Loss\n",
        "=\n",
        "MSE\n",
        "+\n",
        "ùõº\n",
        "‚àë\n",
        "ùë§\n",
        "ùëñ\n",
        "2\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Create model\n",
        "model = Ridge(alpha=1.0)\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "###4. Lasso Regression\n",
        "\n",
        "Purpose: Regression with L1 regularization, encourages sparsity in the coefficients (some coefficients become zero, effectively feature selection).\n",
        "\n",
        "Description: Modifies the loss function by adding the sum of the absolute coefficients:\n",
        "Loss\n",
        "=\n",
        "MSE\n",
        "+\n",
        "ùõº\n",
        "‚àë\n",
        "‚à£\n",
        "ùë§\n",
        "ùëñ\n",
        "‚à£\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "# Create model\n",
        "model = Lasso(alpha=0.1)\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "###5. Elastic Net\n",
        "\n",
        "Purpose: Combines L1 (Lasso) and L2 (Ridge) regularization.\n",
        "\n",
        "Description: Balances between Ridge and Lasso by introducing two hyperparameters,\n",
        "ùõº\n",
        " and\n",
        "ùúå\n",
        ", for controlling the mix.\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "# Create model\n",
        "model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "###6. SGDClassifier and SGDRegressor\n",
        "\n",
        "Purpose: Implements Stochastic Gradient Descent (SGD) for classification and regression.\n",
        "\n",
        "Description: Optimizes models using gradient descent, suitable for large-scale and online learning.\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# Create model\n",
        "model = SGDClassifier(max_iter=1000, tol=1e-3)\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "###7. Perceptron\n",
        "\n",
        "Purpose: Basic linear classifier that updates weights iteratively to separate classes.\n",
        "\n",
        "Description: Works as a simple linear classifier for binary or multi-class problems.\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "# Create model\n",
        "model = Perceptron()\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "###8. BayesianRidge\n",
        "\n",
        "Purpose: Regression model that incorporates Bayesian inference, providing distributions for the coefficients.\n",
        "\n",
        "Description: Useful when estimating uncertainty in the coefficients.\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "\n",
        "# Create model\n",
        "model = BayesianRidge()\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "###9. MultiTaskLasso and MultiTaskElasticNet\n",
        "\n",
        "Purpose: Versions of Lasso and Elastic Net that handle multi-output regression problems.\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "from sklearn.linear_model import MultiTaskLasso\n",
        "\n",
        "# Create model\n",
        "model = MultiTaskLasso(alpha=0.1)\n",
        "model.fit(X_train, Y_train)  # Y_train has multiple outputs\n",
        "```\n",
        "###Advantages of Using sklearn.linear_model\n",
        "\n",
        "1.Simplicity: Easy to implement and understand.\n",
        "\n",
        "2.Flexibility: Offers a wide range of algorithms for linear regression and classification.\n",
        "\n",
        "3.Efficiency: Optimized for speed and can handle large datasets.\n",
        "\n",
        "4.Built-in Tools: Includes regularization, multi-tasking, and probabilistic models.\n",
        "\n",
        "###Conclusion\n",
        "\n",
        "sklearn.linear_model is a versatile module for solving regression and classification problems where linear relationships are involved. Choosing the right model from this module depends on your dataset, problem type, and the need for regularization or feature selection."
      ],
      "metadata": {
        "id": "FQfm8M1iiAaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#18.What does model.fit() do? What arguments must be given?\n",
        "\n",
        "The model.fit() function in Scikit-learn is used to train a machine learning model. It fits the model to the training data by learning the parameters (e.g., weights and biases for linear models) that best represent the underlying relationship between the features and the target variable.\n",
        "\n",
        "###Steps Involved in model.fit()\n",
        "\n",
        "1.Preprocessing Data: Prepares the input data (e.g., scaling or handling categorical variables) if required by the algorithm.\n",
        "\n",
        "2.Compute Gradients (if applicable): For many models, gradients are calculated to minimize the loss function.\n",
        "\n",
        "3.Optimize Parameters: Updates model parameters (e.g., weights in regression or decision boundaries in classification).\n",
        "\n",
        "4.Save the Learned Parameters: Stores the learned model parameters for making predictions with model.predict().\n",
        "\n",
        "###Arguments for model.fit()\n",
        "The specific arguments depend on the type of model, but the general arguments are:\n",
        "\n",
        "###1. X (Required)\n",
        "Description: The feature matrix (input data).\n",
        "Type: A 2D array-like object such as a NumPy array, Pandas DataFrame, or similar.\n",
        "Shape:\n",
        "(\n",
        "ùëõ\n",
        "samples\n",
        ",\n",
        "ùëõ\n",
        "features\n",
        ")\n",
        "ùëõ\n",
        "samples\n",
        " : Number of rows (data points).\n",
        "ùëõ\n",
        "features\n",
        " : Number of columns (features).\n",
        "\n",
        "###2. y (Required)\n",
        "\n",
        "Description: The target variable (output data).\n",
        "Type: A 1D or 2D array-like object.\n",
        "Shape:\n",
        "For regression or binary classification:\n",
        "(\n",
        "ùëõ\n",
        "samples\n",
        ",\n",
        "),\n",
        "For multi-output regression or multi-class classification:\n",
        "(\n",
        "ùëõ\n",
        "samples\n",
        ",\n",
        "ùëõ\n",
        "outputs\n",
        ")\n",
        "\n",
        "###3. Optional Arguments\n",
        "\n",
        "Depending on the model, additional arguments may be required or optional. For example:\n",
        "\n",
        "sample_weight: Array of weights for each sample (used in models like LinearRegression or LogisticRegression).\n",
        "\n",
        "epochs (for neural networks): Number of iterations for training.\n",
        "\n",
        "verbose: Controls the verbosity of the training process.\n",
        "\n",
        "###Example: Linear Regression\n",
        "\n",
        "```\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Data\n",
        "X = np.array([[1], [2], [3], [4]])  # Feature matrix\n",
        "y = np.array([2.5, 5.0, 7.5, 10.0])  # Target variable\n",
        "\n",
        "# Model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# Check learned parameters\n",
        "print(\"Coefficient:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "```\n",
        "\n",
        "###Example: Logistic Regression\n",
        "\n",
        "```\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "# Data\n",
        "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])  # Feature matrix\n",
        "y = np.array([0, 0, 1, 1])  # Target variable (binary classification)\n",
        "\n",
        "# Model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X)\n",
        "print(\"Predictions:\", predictions)\n",
        "```\n",
        "\n",
        "###What Happens After fit()?\n",
        "\n",
        "After calling model.fit():\n",
        "\n",
        "1.The model learns and stores the parameters (e.g., coefficients, decision boundaries, or tree splits).\n",
        "\n",
        "2.You can use model.predict() to make predictions on unseen data.\n",
        "\n",
        "3.You can use methods like model.score() to evaluate the model on test data.\n",
        "\n",
        "###Key Notes\n",
        "\n",
        "1.Mandatory Arguments: At a minimum, you need X (features) and y (target).\n",
        "\n",
        "2.Shape Mismatch: Ensure the number of samples in X matches the number in y.\n",
        "\n",
        "3.Scaling/Preprocessing: Some algorithms require preprocessing before fitting (e.g., scaling for SVMs or logistic regression).\n",
        "\n",
        "By fitting the model to the data, you're enabling it to learn patterns that it can then generalize to unseen data."
      ],
      "metadata": {
        "id": "ogKwFkRNiAWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#19.What does model.predict() do? What arguments must be given?\n",
        "\n",
        "The model.predict() function in Scikit-learn is used to make predictions on new or unseen data after a model has been trained using model.fit(). It takes the feature matrix (input data) as an argument and outputs the predicted values based on the learned parameters of the model.\n",
        "\n",
        "###Key Steps in model.predict()\n",
        "\n",
        "1.Input Validation: Checks whether the input data provided has the correct shape and type.\n",
        "\n",
        "2.Apply Learned Parameters: Uses the parameters (e.g., coefficients and intercept for linear models) that were learned during training to compute predictions.\n",
        "\n",
        "3.Return Predictions: Outputs the predicted values:\n",
        "\n",
        "For regression: Returns continuous values (e.g., sales, temperature).\n",
        "\n",
        "For classification: Returns discrete class labels (e.g., 0, 1, or more for multi-class problems).\n",
        "\n",
        "###Arguments for model.predict()\n",
        "###1. X (Required)\n",
        "Description: The feature matrix (input data) for which predictions are to be made.\n",
        "\n",
        "Type: A 2D array-like object such as a NumPy array, Pandas DataFrame, or similar.\n",
        "\n",
        "Shape:\n",
        "(\n",
        "ùëõ\n",
        "samples\n",
        ",\n",
        "ùëõ\n",
        "features\n",
        ")\n",
        "ùëõ\n",
        "samples\n",
        " : Number of rows (data points).\n",
        "ùëõ\n",
        "features\n",
        " : Number of columns (features).\n",
        "\n",
        "Requirement: The number of features in X must match the number of features used during training.\n",
        "\n",
        "Outputs of model.predict()\n",
        "The output depends on the type of model:\n",
        "\n",
        "\n",
        "Regression Models: Continuous values (e.g., predictions for house prices, stock prices).\n",
        "\n",
        "```\n",
        "# Example output for regression\n",
        "[3.5, 4.2, 6.8]\n",
        "Classification Models: Predicted class labels (e.g., 0, 1, or class names).\n",
        "```\n",
        "\n",
        "```\n",
        "# Example output for classification\n",
        "[0, 1, 1, 0]\n",
        "```\n",
        "\n",
        "###Examples of model.predict()\n",
        "\n",
        "###1. Regression Example\n",
        "\n",
        "```\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Training data\n",
        "X_train = np.array([[1], [2], [3], [4]])  # Feature matrix\n",
        "y_train = np.array([3, 6, 9, 12])  # Target variable\n",
        "\n",
        "# Model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Test data\n",
        "X_test = np.array([[5], [6]])\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Predictions:\", y_pred)\n",
        "# Output: [15, 18]\n",
        "```\n",
        "\n",
        "###2. Classification Example\n",
        "\n",
        "```\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "# Training data\n",
        "X_train = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])  # Feature matrix\n",
        "y_train = np.array([0, 0, 1, 1])  # Target variable\n",
        "\n",
        "# Model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Test data\n",
        "X_test = np.array([[3, 3], [5, 6]])\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Predictions:\", y_pred)\n",
        "# Output: [0, 1]\n",
        "```\n",
        "\n",
        "###Key Notes on model.predict()\n",
        "\n",
        "1.Mandatory Argument: You must provide the feature matrix X with the same number of features as the training data.\n",
        "\n",
        "2.No Labels Needed: Unlike model.fit(), the target variable y is not required.\n",
        "\n",
        "3.Shape Compatibility: The test data X must have the same feature dimensions as the training data.\n",
        "\n",
        "4.Preprocessing Consistency: If preprocessing (e.g., scaling or encoding) was applied to the training data, the same transformations must be applied to the test data before using model.predict().\n",
        "\n",
        "###What Happens Internally in model.predict()?\n",
        "\n",
        "1.Linear Regression:\n",
        "For a feature vector\n",
        "ùëã\n",
        ", predictions are computed as:\n",
        "\n",
        "ùë¶\n",
        "pred\n",
        "=\n",
        "ùëã\n",
        "‚ãÖ\n",
        "ùë§\n",
        "+\n",
        "ùëè\n",
        "where\n",
        "w are the weights and\n",
        "\n",
        "ùëè is the bias/intercept.\n",
        "\n",
        "\n",
        "2.Logistic Regression:\n",
        "\n",
        "Computes probabilities first using the sigmoid function, and then maps them to discrete class labels.\n",
        "\n",
        "3.Tree-based Models:\n",
        "\n",
        "Traverses the decision tree splits to predict the output.\n",
        "\n",
        "###Conclusion\n",
        "model.predict() is a key function for making predictions after training. It applies the learned model to new data and outputs the predicted values or classes, which can then be used for evaluation or decision-making."
      ],
      "metadata": {
        "id": "GgO_GhAqiASO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#20.What are continuous and categorical variables?\n",
        "\n",
        "Continuous and Categorical Variables\n",
        "In statistics and machine learning, variables represent different features or attributes of data. They are generally classified into continuous and categorical variables based on the nature of their values.\n",
        "\n",
        "###1. Continuous Variables\n",
        "A continuous variable can take an infinite number of values within a given range. These variables are typically numerical and can be measured but not counted.\n",
        "\n",
        "###Characteristics:\n",
        "Values are real numbers. Can take fractional or decimal values. Represent measurable quantities like height, weight, or temperature.\n",
        "\n",
        "###Examples:\n",
        "Height in centimeters (e.g., 172.5 cm).\n",
        "\n",
        "Temperature in degrees Celsius (e.g., 22.4¬∞C).\n",
        "\n",
        "Age in years (e.g., 25.8 years).\n",
        "\n",
        "###Usage in Machine Learning:\n",
        "Continuous variables are often used in regression tasks.\n",
        "\n",
        "Require normalization or standardization for models like neural networks or k-NN.\n",
        "\n",
        "###2. Categorical Variables\n",
        "A categorical variable has a finite set of distinct values or categories. These variables represent qualitative data and are often used to classify data into groups.\n",
        "\n",
        "###Characteristics:\n",
        "Values are discrete and cannot be divided into fractions.\n",
        "\n",
        "Represent labels or groups.\n",
        "\n",
        "Categories can be nominal (no inherent order) or ordinal (ordered).\n",
        "\n",
        "###Examples:\n",
        "Nominal: Colors (red, blue, green).\n",
        "\n",
        "Gender (male, female, other).\n",
        "\n",
        "Ordinal: Education level (high school, bachelor's, master's, Ph.D.).\n",
        "\n",
        "Customer satisfaction (poor, average, excellent).\n",
        "\n",
        "###Usage in Machine Learning:\n",
        "\n",
        "Used in classification tasks.\n",
        "\n",
        "Need to be encoded numerically (e.g., one-hot encoding, label encoding)."
      ],
      "metadata": {
        "id": "6EiKqVjziAOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#21.What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "Feature scaling is the process of normalizing or standardizing the range of features (independent variables) in a dataset. It ensures that all features contribute equally to the model, avoiding bias caused by varying scales or magnitudes.\n",
        "\n",
        "###Why Is Feature Scaling Important?\n",
        "\n",
        "Many machine learning algorithms compute distances, gradients, or weights during their training process. If the features have different scales, some may dominate over others, leading to suboptimal performance. Feature scaling ensures fair contribution and faster convergence in optimization.\n",
        "\n",
        "###Examples of Scenarios Requiring Feature Scaling\n",
        "\n",
        "###1.Distance-based Algorithms:\n",
        "\n",
        "Algorithms like K-Nearest Neighbors (KNN), Support Vector Machines (SVMs), and K-Means clustering rely on distance metrics such as Euclidean distance.\n",
        "\n",
        "Features with larger scales can disproportionately affect these distances.\n",
        "\n",
        "###2.Gradient-based Optimization:\n",
        "\n",
        "In algorithms like Logistic Regression or Neural Networks, larger feature values can lead to slower convergence during gradient descent.\n",
        "\n",
        "###3.Regularization Techniques:\n",
        "\n",
        "In regularized models like Ridge and Lasso regression, feature magnitudes can impact the regularization penalty.\n",
        "\n",
        "###Types of Feature Scaling\n",
        "\n",
        "###1. Standardization (Z-Score Normalization)\n",
        "Formula:\n",
        "\n",
        "ùëß\n",
        "=\n",
        "ùë•\n",
        "‚àí\n",
        "ùúá\n",
        "/\n",
        "ùúé\n",
        "\n",
        "\n",
        "where:\n",
        "\n",
        "ùë•: Original value.\n",
        "\n",
        "ùúá: Mean of the feature.\n",
        "\n",
        "ùúé: Standard deviation of the feature.\n",
        "\n",
        "Effect: Scales data to have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "Use Case: Commonly used in gradient-based algorithms like Logistic Regression or Neural Networks.\n",
        "\n",
        "###2. Min-Max Normalization (Rescaling)\n",
        "Formula:\n",
        "\n",
        "ùë•\n",
        "scaled\n",
        "=\n",
        "ùë•\n",
        "‚àí\n",
        "min\n",
        "(\n",
        "ùë•\n",
        ")\n",
        "/\n",
        "max\n",
        "(\n",
        "ùë•\n",
        ")\n",
        "‚àí\n",
        "min\n",
        "(\n",
        "ùë•\n",
        ")\n",
        "\n",
        "Effect: Scales data to a fixed range, usually [0, 1].\n",
        "\n",
        "Use Case: Useful for algorithms like KNN or Neural Networks when data needs to be normalized to a specific range.\n",
        "\n",
        "###3. Robust Scaling\n",
        "Formula:\n",
        "\n",
        "ùë•\n",
        "scaled\n",
        "=\n",
        "ùë•\n",
        "‚àí\n",
        "median\n",
        "(\n",
        "ùë•\n",
        ")\n",
        "/\n",
        "IQR\n",
        "(\n",
        "ùë•\n",
        ")\n",
        "\n",
        "\n",
        "where\n",
        "IQR is the interquartile range (75th percentile - 25th percentile).\n",
        "\n",
        "Effect: Scales data based on the median and IQR, making it robust to outliers.\n",
        "\n",
        "Use Case: Suitable when the dataset contains outliers.\n",
        "\n",
        "###4. MaxAbs Scaling\n",
        "Formula:\n",
        "\n",
        "ùë•\n",
        "scaled\n",
        "=\n",
        "ùë•\n",
        "/\n",
        "max\n",
        "(\n",
        "‚à£\n",
        "ùë•\n",
        "‚à£\n",
        ")\n",
        "\n",
        "Effect: Scales data by dividing by the maximum absolute value. Data is transformed into the range [-1, 1].\n",
        "\n",
        "Use Case: Used when the dataset is sparse (many zeros).\n",
        "\n",
        "###Benefits of Feature Scaling\n",
        "\n",
        "1.Improves Convergence: Faster convergence of gradient-based optimization methods.\n",
        "\n",
        "2.Ensures Fairness: Features with larger magnitudes do not dominate the learning process.\n",
        "\n",
        "3.Enhances Accuracy: Better results for distance-based algorithms.\n",
        "\n",
        "4.Handles Regularization Properly: Prevents one feature from being penalized disproportionately.\n",
        "\n",
        "###Feature Scaling in Python (Using Scikit-learn)\n",
        "Scikit-learn provides several methods for feature scaling:\n",
        "\n",
        "###Standardization (Using StandardScaler)\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Data\n",
        "X = np.array([[1, 200], [2, 300], [3, 400]])\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(X_scaled)\n",
        "```\n",
        "\n",
        "###Min-Max Scaling (Using MinMaxScaler)\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Data\n",
        "X = np.array([[1, 200], [2, 300], [3, 400]])\n",
        "\n",
        "# Scaling\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(X_scaled)\n",
        "```\n",
        "\n",
        "###Robust Scaling (Using RobustScaler)\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Data\n",
        "X = np.array([[1, 200], [2, 300], [100, 400]])\n",
        "\n",
        "# Scaling\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(X_scaled)\n",
        "```\n",
        "\n",
        "###When Not to Use Feature Scaling\n",
        "\n",
        "1.Tree-based Models: Algorithms like Decision Trees, Random Forests, and Gradient Boosted Trees are not sensitive to feature scaling because they split data based on thresholds.\n",
        "\n",
        "2.Already Normalized Data: If the data is already scaled or in the same units, further scaling might not be necessary.\n",
        "\n",
        "###Conclusion\n",
        "\n",
        "Feature scaling is a crucial preprocessing step that ensures all features contribute equally, leading to better performance and faster convergence for many machine learning algorithms. Choosing the right scaling method depends on the dataset characteristics and the algorithm being used."
      ],
      "metadata": {
        "id": "UJsj-X8miAIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#22.How do we perform scaling in Python?\n",
        "\n",
        "In Python, scaling is typically performed using the sklearn.preprocessing module, which provides various scalers for different scaling methods. Here‚Äôs how to perform feature scaling step-by-step:\n",
        "\n",
        "###1. Standardization (Z-Score Normalization)\n",
        "Scales data to have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "Code Example\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "X = np.array([[1, 200], [2, 300], [3, 400]])\n",
        "\n",
        "# Standardization\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"Standardized Data:\\n\", X_scaled)\n",
        "```\n",
        "\n",
        "###2. Min-Max Scaling (Normalization)\n",
        "\n",
        "Scales data to a fixed range, typically [0, 1].\n",
        "\n",
        "Code Example\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Example data\n",
        "X = np.array([[1, 200], [2, 300], [3, 400]])\n",
        "\n",
        "# Min-Max Scaling\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"Min-Max Scaled Data:\\n\", X_scaled)\n",
        "```\n",
        "\n",
        "###3. Robust Scaling\n",
        "\n",
        "Scales data using the median and interquartile range (IQR).\n",
        "\n",
        "Effective for datasets with outliers.\n",
        "\n",
        "Code Example\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Example data\n",
        "X = np.array([[1, 200], [2, 300], [100, 400]])\n",
        "\n",
        "# Robust Scaling\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"Robust Scaled Data:\\n\", X_scaled)\n",
        "```\n",
        "\n",
        "###4. MaxAbs Scaling\n",
        "\n",
        "Scales data by dividing each feature by its maximum absolute value.\n",
        "\n",
        "Preserves sparsity (used for sparse datasets).\n",
        "\n",
        "Code Example\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "\n",
        "# Example data\n",
        "X = np.array([[1, 200], [2, 300], [3, 400]])\n",
        "\n",
        "# MaxAbs Scaling\n",
        "scaler = MaxAbsScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"MaxAbs Scaled Data:\\n\", X_scaled)\n",
        "```\n",
        "\n",
        "###5. Custom Scaling (Using NumPy)\n",
        "\n",
        "Manually perform scaling using mathematical formulas.\n",
        "\n",
        "Code Example for Min-Max Scaling\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "X = np.array([[1, 200], [2, 300], [3, 400]])\n",
        "\n",
        "# Min-Max Scaling manually\n",
        "X_min = X.min(axis=0)\n",
        "X_max = X.max(axis=0)\n",
        "\n",
        "X_scaled = (X - X_min) / (X_max - X_min)\n",
        "\n",
        "print(\"Custom Scaled Data:\\n\", X_scaled)\n",
        "```\n",
        "\n",
        "###6. Scaling with Pandas\n",
        "\n",
        "Apply scaling directly to Pandas DataFrames.\n",
        "\n",
        "Code Example\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Example data\n",
        "data = {'Feature1': [1, 2, 3], 'Feature2': [200, 300, 400]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Min-Max Scaling\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
        "\n",
        "print(\"Scaled DataFrame:\\n\", df_scaled)\n",
        "```\n",
        "\n",
        "###Key Notes\n",
        "###1.Fit vs. Fit-Transform:\n",
        "\n",
        "scaler.fit(X): Learns scaling parameters (e.g., mean and standard deviation for StandardScaler).\n",
        "\n",
        "scaler.transform(X): Applies the learned scaling parameters.\n",
        "\n",
        "scaler.fit_transform(X): Combines both steps.\n",
        "\n",
        "###2.Consistent Scaling:\n",
        "\n",
        "Always use the same scaler fitted on training data to scale test data.\n",
        "Example:\n",
        "\n",
        "```\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "```\n",
        "\n",
        "###3.Pipeline Integration:\n",
        "\n",
        "Scaling can be combined with model training using Pipeline:\n",
        "\n",
        "```\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', LogisticRegression())\n",
        "])\n",
        "pipeline.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "By using these methods, you can scale your features effectively in Python, ensuring your machine learning models perform optimally."
      ],
      "metadata": {
        "id": "Ngb3PGdYiAFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#23.What is sklearn.preprocessing?\n",
        "\n",
        "sklearn.preprocessing is a module in the scikit-learn library in Python that provides tools and functions for data preprocessing. Preprocessing is a crucial step in machine learning to prepare raw data for modeling. It involves scaling, encoding, normalizing, and transforming data to make it suitable for machine learning algorithms.\n",
        "\n",
        "###Key Features of sklearn.preprocessing\n",
        "\n",
        "1.Data Scaling and Normalization:\n",
        "\n",
        "Adjusts data to ensure that features have a consistent scale or distribution, which is essential for algorithms sensitive to feature magnitudes.\n",
        "\n",
        "2.Encoding Categorical Variables:\n",
        "\n",
        "Transforms non-numeric (categorical) data into numerical formats for machine learning models.\n",
        "\n",
        "3.Imputation and Transformation:\n",
        "\n",
        "Handles missing values and applies mathematical transformations to features.\n",
        "\n",
        "4.Feature Selection:\n",
        "\n",
        "Prepares specific features based on user-defined transformations.\n",
        "\n",
        "###Commonly Used Classes and Functions\n",
        "1.Scaling\n",
        "\n",
        "StandardScaler:\n",
        "\n",
        "Scales data to have a mean of 0 and a standard deviation of 1 (z-score normalization).\n",
        "Useful for algorithms like SVM, logistic regression, and neural networks.\n",
        "```\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "```\n",
        "MinMaxScaler:\n",
        "\n",
        "Scales data to a fixed range, typically\n",
        "[\n",
        "0\n",
        ",\n",
        "1\n",
        "].\n",
        "Preserves the shape of the distribution.\n",
        "```\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "```\n",
        "MaxAbsScaler:\n",
        "\n",
        "Scales data to\n",
        "[\n",
        "‚àí\n",
        "1\n",
        ",\n",
        "1\n",
        "]based on the maximum absolute value of each feature.\n",
        "```\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "scaler = MaxAbsScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "```\n",
        "\n",
        "2.Normalization\n",
        "\n",
        "Normalizes feature vectors to have a unit norm (useful for text classification or clustering).\n",
        "```\n",
        "from sklearn.preprocessing import Normalizer\n",
        "normalizer = Normalizer()\n",
        "normalized_data = normalizer.fit_transform(data)\n",
        "```\n",
        "3. Encoding\n",
        "\n",
        "LabelEncoder:\n",
        "\n",
        "Encodes categorical labels into integers.\n",
        "```\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "encoded_labels = encoder.fit_transform(labels)\n",
        "```\n",
        "OneHotEncoder:\n",
        "\n",
        "Converts categorical features into a binary matrix.\n",
        "```\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder()\n",
        "encoded_features = encoder.fit_transform(features).toarray()\n",
        "```\n",
        "4.Binarization\n",
        "\n",
        "Converts numerical values into binary (0 or 1) based on a threshold.\n",
        "```\n",
        "from sklearn.preprocessing import Binarizer\n",
        "binarizer = Binarizer(threshold=5.0)\n",
        "binary_data = binarizer.fit_transform(data)\n",
        "```\n",
        "\n",
        "5.Polynomial Features\n",
        "\n",
        "Generates new features by computing combinations of existing features.\n",
        "```\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "poly_features = poly.fit_transform(data)\n",
        "```\n",
        "6. Power Transformation\n",
        "\n",
        "Stabilizes variance and makes data more Gaussian-like.\n",
        "\n",
        "PowerTransformer: Applies power transformations like Box-Cox or Yeo-Johnson.\n",
        "```\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "pt = PowerTransformer()\n",
        "transformed_data = pt.fit_transform(data)\n",
        "```\n",
        "\n",
        "###Why Use sklearn.preprocessing?\n",
        "\n",
        "1.Improves Model Performance:\n",
        "Ensures data is on a compatible scale for machine learning algorithms.\n",
        "\n",
        "2.Handles Different Data Types:\n",
        "Encodes categorical and continuous data for compatibility with numerical models.\n",
        "\n",
        "3.Prepares Data for Robustness:\n",
        "Mitigates issues like outliers, skewness, or inconsistent feature magnitudes.\n",
        "\n",
        "Using sklearn.preprocessing, you can seamlessly preprocess your data, ensuring it is clean and ready for model training and evaluation."
      ],
      "metadata": {
        "id": "7o6iemdniABB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#24.How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "In machine learning, the data is typically split into training and test sets to evaluate the model's ability to generalize to unseen data. The training set is used to train the model, while the test set is used to evaluate its performance.\n",
        "\n",
        "Here's a step-by-step guide to splitting data in Python using scikit-learn:\n",
        "\n",
        "1.Import Necessary Libraries python Copy code from sklearn.model_selection import train_test_split.\n",
        "\n",
        "2.Prepare Your Data Assume you have a dataset with features (X) and target labels (y):\n",
        "\n",
        "```\n",
        "X = [[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7]]  # Example features\n",
        "y = [0, 1, 0, 1, 0, 1]  # Example labels\n",
        "```\n",
        "\n",
        "3.Split the Data Use train_test_split to split the dataset into training and testing sets:\n",
        "\n",
        "```\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "```\n",
        "\n",
        "Here:\n",
        "\n",
        "test_size=0.2: 20% of the data will be used for testing, and the remaining 80% will be used for training. random_state=42: Ensures that the split is reproducible (i.e., you get the same split every time you run the code).\n",
        "\n",
        "4.Confirm the Split You can print the sizes of the splits to confirm:\n",
        "\n",
        "```\n",
        "print(\"Training features shape:\", len(X_train))\n",
        "print(\"Testing features shape:\", len(X_test))\n",
        "```\n"
      ],
      "metadata": {
        "id": "ELEmQMMSh_8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#25.Explain data encoding?\n",
        "\n",
        "Data encoding is the process of converting categorical data (non-numerical data) into numerical formats so that it can be used by machine learning models. Most algorithms in machine learning require numerical inputs, so encoding is essential when dealing with categorical features.\n",
        "\n",
        "###Types of Data\n",
        "1.Categorical Data:\n",
        "\n",
        "Data with discrete categories (e.g., \"Red\", \"Blue\", \"Green\").\n",
        "\n",
        "Can be Nominal (no order) or Ordinal (has a natural order).\n",
        "\n",
        "2.Numerical Data:\n",
        "\n",
        "Continuous or discrete numerical values (e.g., 10, 20, 30).\n",
        "\n",
        "###Why Encoding is Important?\n",
        "\n",
        "Machine learning algorithms cannot process strings or categorical values directly.\n",
        "\n",
        "Encoding helps in converting such data into a numerical format while preserving the information or relationships.\n",
        "\n",
        "###Types of Data Encoding\n",
        "\n",
        "###1. Label Encoding\n",
        "\n",
        "What it does: Assigns a unique integer to each category in the data.\n",
        "\n",
        "Use case: Useful for ordinal data where the order matters (e.g., \"Low\", \"Medium\", \"High\").\n",
        "\n",
        "Implementation:\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Data\n",
        "data = ['Low', 'Medium', 'High', 'Medium']\n",
        "\n",
        "# Encoding\n",
        "encoder = LabelEncoder()\n",
        "encoded_data = encoder.fit_transform(data)\n",
        "\n",
        "print(\"Encoded Data:\", encoded_data)\n",
        "# Output: [1, 2, 0, 2]\n",
        "```\n",
        "\n",
        "###2. One-Hot Encoding\n",
        "\n",
        "What it does: Converts categories into binary vectors with 1 for the present category and 0 for the others.\n",
        "\n",
        "Use case: Useful for nominal data where there‚Äôs no order (e.g., \"Red\", \"Blue\", \"Green\").\n",
        "\n",
        "Implementation:\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Data\n",
        "data = np.array(['Red', 'Blue', 'Green']).reshape(-1, 1)\n",
        "\n",
        "# Encoding\n",
        "encoder = OneHotEncoder()\n",
        "encoded_data = encoder.fit_transform(data).toarray()\n",
        "\n",
        "print(\"One-Hot Encoded Data:\\n\", encoded_data)\n",
        "# Output:\n",
        "# [[1. 0. 0.]\n",
        "#  [0. 1. 0.]\n",
        "#  [0. 0. 1.]]\n",
        "```\n",
        "\n",
        "###3. Ordinal Encoding\n",
        "\n",
        "What it does: Assigns integers to categories in an ordinal way based on their order.\n",
        "\n",
        "Use case: For ordinal data where the order is meaningful.\n",
        "\n",
        "Implementation:\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Data\n",
        "data = np.array(['Low', 'Medium', 'High']).reshape(-1, 1)\n",
        "\n",
        "# Encoding\n",
        "encoder = OrdinalEncoder(categories=[['Low', 'Medium', 'High']])\n",
        "encoded_data = encoder.fit_transform(data)\n",
        "\n",
        "print(\"Ordinal Encoded Data:\\n\", encoded_data)\n",
        "# Output: [[0.]\n",
        "#          [1.]\n",
        "#          [2.]]\n",
        "```\n",
        "\n",
        "###4. Frequency Encoding\n",
        "\n",
        "What it does: Replaces each category with its frequency or count in the dataset.\n",
        "\n",
        "Use case: Effective when the frequency of categories carries information.\n",
        "\n",
        "Implementation:\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "\n",
        "# Data\n",
        "data = pd.Series(['A', 'B', 'A', 'C', 'B', 'A'])\n",
        "\n",
        "# Frequency Encoding\n",
        "encoded_data = data.map(data.value_counts(normalize=False))\n",
        "print(\"Frequency Encoded Data:\\n\", encoded_data)\n",
        "# Output: [3, 2, 3, 1, 2, 3]\n",
        "```\n",
        "\n",
        "###5. Target/Mean Encoding\n",
        "\n",
        "What it does: Replaces each category with the mean of the target variable for that category.\n",
        "\n",
        "Use case: Often used in categorical features when training data is large.\n",
        "\n",
        "Implementation:\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "\n",
        "# Data\n",
        "df = pd.DataFrame({'Category': ['A', 'B', 'A', 'C', 'B', 'A'], 'Target': [1, 0, 1, 1, 0, 1]})\n",
        "\n",
        "# Target Encoding\n",
        "encoded_data = df.groupby('Category')['Target'].transform('mean')\n",
        "print(\"Target Encoded Data:\\n\", encoded_data)\n",
        "# Output: [1.0, 0.0, 1.0, 1.0, 0.0, 1.0]\n",
        "```\n",
        "\n",
        "###6. Binary Encoding\n",
        "\n",
        "What it does: Converts categories into binary numbers and represents them as separate columns.\n",
        "\n",
        "Use case: Useful for large numbers of categories to reduce dimensionality.\n",
        "\n",
        "Implementation:\n",
        "\n",
        "```\n",
        "from category_encoders import BinaryEncoder\n",
        "import pandas as pd\n",
        "\n",
        "# Data\n",
        "data = pd.DataFrame({'Category': ['A', 'B', 'C']})\n",
        "\n",
        "# Encoding\n",
        "encoder = BinaryEncoder(cols=['Category'])\n",
        "encoded_data = encoder.fit_transform(data)\n",
        "\n",
        "print(\"Binary Encoded Data:\\n\", encoded_data)\n",
        "# Output:\n",
        "#    Category_0  Category_1\n",
        "# 0           0           1\n",
        "# 1           1           0\n",
        "# 2           1           1\n",
        "```\n",
        "Data encoding is essential to preprocess categorical variables for machine learning models. The choice of encoding technique depends on the type of categorical data (nominal or ordinal) and the requirements of the algorithm. Proper encoding ensures that the information is preserved while making the data usable by machine learning models."
      ],
      "metadata": {
        "id": "A748sNYah_wG"
      }
    }
  ]
}