{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Theoretical Questions"
      ],
      "metadata": {
        "id": "0CRyUU-6Dhtm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.Can we use Bagging for regression problems.\n",
        "\n",
        "Yes, Bagging can be used for regression problems. The Bagging Regressor is similar to the Bagging Classifier but is used for regression tasks. It trains multiple models on different bootstrap samples of the dataset and then averages their predictions. This reduces variance, making the model more stable and less prone to overfitting. It is particularly useful for high-variance models like Decision Trees.\n",
        "\n"
      ],
      "metadata": {
        "id": "nMqs-T8GDksh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.What is the difference between multiple model training and single model training?\n",
        "\n",
        "**Single Model Training:** A single model is trained on the entire dataset. If the model is complex, it may overfit, and if it is simple, it may underfit.\n",
        "\n",
        "**Multiple Model Training (Ensemble Learning):** Multiple models are trained and their predictions are combined. This improves accuracy, reduces bias and variance, and makes predictions more robust. Examples include Bagging, Boosting, and Stacking."
      ],
      "metadata": {
        "id": "FuHbe-PEEWJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.Explain the concept of feature randomness in Random Forest.\n",
        "\n",
        "Feature randomness in Random Forest refers to the process of selecting a random subset of features at each split when constructing decision trees. Unlike standard decision trees that consider all available features at every split, Random Forest introduces randomness by choosing only a subset of features, which reduces correlation between trees and improves generalization.\n",
        "\n",
        "###How Feature Randomness Works:\n",
        "1.Random Feature Selection at Each Split\n",
        "\n",
        "When a tree needs to decide on a split, it does not consider all features but instead randomly selects a subset of features.\n",
        "\n",
        "This ensures that different trees focus on different aspects of the data.\n",
        "\n",
        "2.Reduces Overfitting\n",
        "\n",
        "Decision trees are prone to overfitting, especially when they rely heavily on a few dominant features.\n",
        "\n",
        "By introducing randomness, trees are less dependent on specific features, making the model more robust.\n",
        "\n",
        "3.Creates Diversity Among Trees\n",
        "\n",
        "Since each tree is built using a different set of features and trained on a different subset of data (due to bootstrap sampling), the trees make different errors.\n",
        "\n",
        "This diversity helps reduce variance when their predictions are averaged.\n",
        "\n",
        "###Key Benefits of Feature Randomness:\n",
        "✔ Reduces overfitting compared to single Decision Trees.\n",
        "\n",
        "✔ Improves model generalization to new data.\n",
        "\n",
        "✔ Encourages diverse trees, making the ensemble more effective."
      ],
      "metadata": {
        "id": "v1EqhhtrEjSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.What is OOB (Out-of-Bag) Score?\n",
        "\n",
        "The Out-of-Bag (OOB) Score is an internal validation technique used in Bagging-based models, such as Random Forest. It provides an unbiased estimate of the model’s performance without requiring a separate validation set.\n",
        "\n",
        "###How OOB Score Works:\n",
        "1.Bootstrap Sampling\n",
        "\n",
        "In Bagging, each tree is trained on a random subset of the dataset (with replacement).\n",
        "\n",
        "Some data points are left out in each bootstrap sample (these are called \"Out-of-Bag\" samples).\n",
        "\n",
        "2.OOB Prediction\n",
        "\n",
        "Once the Random Forest is trained, each tree makes predictions for only the data points it has not seen during training (i.e., its OOB samples).\n",
        "\n",
        "The final prediction for a sample is obtained by aggregating predictions from all trees that did not use that sample for training.\n",
        "\n",
        "3.OOB Score Calculation\n",
        "\n",
        "The model’s performance is evaluated using these OOB predictions and compared with the actual labels.\n",
        "\n",
        "It is typically measured using accuracy (classification) or R² score (regression).\n",
        "\n",
        "###Advantages of OOB Score:\n",
        "\n",
        "✔ No need for a separate validation set, saving data for training.\n",
        "\n",
        "✔ Provides an unbiased estimate of model performance.\n",
        "\n",
        "✔ Helps detect overfitting in Random Forest models.\n",
        "\n",
        "###When to Use OOB Score?\n",
        "\n",
        "When the dataset is small and you want to avoid using a separate validation set.\n",
        "\n",
        "When training a Random Forest or other Bagging-based models.\n",
        "\n",
        "When you need an unbiased performance estimate without cross-validation.\n"
      ],
      "metadata": {
        "id": "idvw3JYvFKRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.How can you measure the importance of features in a Random Forest model?\n",
        "\n",
        "Feature importance in Random Forest helps identify which input variables have the most impact on the model's predictions. Random Forest provides two common ways to measure feature importance:\n",
        "\n",
        "1. Gini Importance (Mean Decrease in Impurity - MDI)\n",
        "\n",
        "This method calculates how much each feature reduces impurity (Gini impurity for classification or variance for regression) in the decision trees.\n",
        "Features that contribute more to node splits across many trees are considered more important.\n",
        "\n",
        "It is computed as the average decrease in impurity weighted by the number of samples affected by each split.\n",
        "\n",
        "2. Permutation Importance (Mean Decrease in Accuracy - MDA)\n",
        "\n",
        "This method measures feature importance by randomly shuffling the values of each feature and checking how much the model's performance decreases.\n",
        "\n",
        "A greater drop in accuracy means the feature is important.\n",
        "\n",
        "Unlike Gini Importance, it works better when features are correlated.\n",
        "\n",
        "###Key Takeaways:\n",
        "\n",
        "✔ Gini Importance is faster but biased when features are correlated.\n",
        "\n",
        "✔ Permutation Importance is more reliable but computationally expensive.\n",
        "\n",
        "✔ Feature importance helps in feature selection, reducing dimensionality and improving model performance."
      ],
      "metadata": {
        "id": "2m47pDwtFhM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6.Explain the working principle of a Bagging Classifier.\n",
        "\n",
        "A Bagging Classifier (Bootstrap Aggregating Classifier) is an ensemble learning method that improves the accuracy and stability of machine learning models by combining multiple weak learners (e.g., Decision Trees). It works by training multiple models on different subsets of the training data and aggregating their predictions.\n",
        "\n",
        "###Working Steps of a Bagging Classifier\n",
        "\n",
        "###1. Bootstrap Sampling (Data Resampling)\n",
        "From the original dataset, multiple random subsets (with replacement) are created.\n",
        "\n",
        "Each subset is used to train a separate base model.\n",
        "\n",
        "###2. Model Training\n",
        "\n",
        "Each model (usually Decision Trees) is trained independently on its corresponding subset.\n",
        "\n",
        "This creates diverse models that learn different patterns in the data.\n",
        "\n",
        "###3. Aggregation of Predictions\n",
        "Once all models are trained, their predictions are combined to get the final output:\n",
        "\n",
        "For Classification → Majority Voting (the class predicted by most models is chosen).\n",
        "\n",
        "For Regression → Averaging (the mean of all predictions is taken).\n",
        "\n",
        "4. Final Prediction\n",
        "The aggregated prediction is more accurate, stable, and less prone to overfitting than a single model.\n",
        "\n",
        "###Advantages of a Bagging Classifier\n",
        "\n",
        "✔ Reduces Overfitting – By training on different subsets, it avoids memorizing noise.\n",
        "\n",
        "✔ Improves Accuracy – The final prediction is more robust than a single model.\n",
        "\n",
        "✔ Handles High Variance Models – Works well with models prone to overfitting (e.g., Decision Trees).\n",
        "\n"
      ],
      "metadata": {
        "id": "B9JvuWTtGEIB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7.How do you evaluate a Bagging Classifier’s performance.\n",
        "\n",
        "To evaluate a Bagging Classifier, use metrics like accuracy, precision, recall, and F1-score for classification tasks. Use cross-validation to get an unbiased estimate. The model can also be compared to a single Decision Tree to check improvement.\n",
        "\n"
      ],
      "metadata": {
        "id": "LGdnx8KpGsTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###8.How does a Bagging Regressor work.\n",
        "\n",
        "A Bagging Regressor trains multiple models (e.g., Decision Trees) on different bootstrap samples. Instead of majority voting (used in classification), it averages the predictions of all models to get the final output. This averaging reduces variance, leading to a more stable and accurate prediction.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k24Oq_U8G3-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###9.What is the main advantage of ensemble techniques?\n",
        "\n",
        "The main advantage of ensemble techniques is that they improve model accuracy and robustness by combining multiple models, reducing bias, variance, and overfitting.\n",
        "\n",
        "###Key Benefits of Ensemble Techniques:\n",
        "\n",
        "1. Higher Accuracy\n",
        "\n",
        "Combining multiple models helps achieve better predictions than a single weak learner.\n",
        "\n",
        "Models correct each other's errors, improving overall performance.\n",
        "\n",
        "2. Reduces Overfitting (Lower Variance)\n",
        "\n",
        "Bagging techniques (like Random Forest) train models on different data subsets, reducing model variance.\n",
        "\n",
        "This prevents the model from memorizing noise in the training set.\n",
        "\n",
        "3. Reduces Bias (Better Generalization)\n",
        "\n",
        "Boosting techniques (like XGBoost, AdaBoost) focus on misclassified instances, improving learning efficiency.\n",
        "This lowers bias and enhances predictive power.\n",
        "\n",
        "4. Works Well with Weak Models\n",
        "\n",
        "Even weak classifiers (e.g., Decision Trees, Logistic Regression) can be combined into a powerful ensemble model.\n",
        "\n",
        "5. More Stable and Reliable Predictions\n",
        "\n",
        "Aggregating multiple models ensures that predictions remain consistent even when the dataset has variations.\n"
      ],
      "metadata": {
        "id": "1353o8jUHAcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###10.What is the main challenge of ensemble methods.\n",
        "\n",
        "While ensemble methods improve accuracy and reduce overfitting, they also come with certain challenges. The main challenge of ensemble methods is their higher computational cost and complexity, as they involve training multiple models instead of just one.\n",
        "\n",
        "###Key Challenges of Ensemble Methods\n",
        "1. High Computational Cost\n",
        "\n",
        "Training multiple models requires more processing power and memory compared to a single model.\n",
        "\n",
        "Large ensembles (e.g., Random Forest with hundreds of trees or Boosting models) can take a long time to train.\n",
        "2. Difficult to Interpret\n",
        "\n",
        "Unlike simple models (e.g., Decision Trees or Linear Regression), ensemble models are black boxes and hard to interpret.\n",
        "\n",
        "This is a major issue in applications where explainability is crucial, such as healthcare and finance.\n",
        "\n",
        "3. Risk of Overfitting in Some Cases\n",
        "\n",
        "While Bagging reduces variance, Boosting can sometimes overfit, especially if too many weak learners are added.\n",
        "\n",
        "Careful tuning of hyperparameters (e.g., learning rate, number of estimators) is needed to balance bias and variance.\n",
        "\n",
        "4. Need for More Data\n",
        "\n",
        "Ensembles work best when diverse models are trained on large datasets.\n",
        "On small datasets, using an ensemble might not significantly improve accuracy.\n",
        "\n",
        "5. Complex Hyperparameter Tuning\n",
        "\n",
        "Models like Random Forest, XGBoost, and Stacking have multiple parameters that must be optimized (e.g., number of trees, depth, learning rate).\n",
        "This makes hyperparameter tuning time-consuming and resource-intensive."
      ],
      "metadata": {
        "id": "fUPx5oF0Hcys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###11.Explain the key idea behind ensemble techniques.\n",
        "\n",
        "The key idea behind ensemble techniques is to combine multiple weak models (learners) to create a stronger and more accurate model. Instead of relying on a single model, ensemble learning takes the predictions of multiple models and aggregates them to improve accuracy, stability, and generalization.\n",
        "\n",
        "###Why Use Ensemble Techniques?\n",
        "\n",
        "A single model can overfit or underfit, depending on its complexity.\n",
        "\n",
        "Ensemble methods use diverse models to reduce bias, variance, and noise in predictions.\n",
        "\n",
        "They ensure that errors from individual models do not dominate the final prediction.\n",
        "\n",
        "###How Ensemble Learning Works:\n",
        "\n",
        "1.Train Multiple Models\n",
        "\n",
        "Use the same dataset but train different models or multiple versions of the same model.\n",
        "\n",
        "2.Aggregate Predictions\n",
        "\n",
        "Combine results from all models using majority voting (classification) or averaging (regression).\n",
        "\n",
        "3.Final Decision\n",
        "\n",
        "The final prediction is more accurate and stable than any single model.\n",
        "\n",
        "###Types of Ensemble Techniques\n",
        "1.Bagging (Bootstrap Aggregating)\n",
        "Reduces variance by training models on different random subsets of the data.\n",
        "\n",
        "Example: Random Forest (multiple Decision Trees trained independently).\n",
        "\n",
        "2.Boosting\n",
        "\n",
        "Reduces bias by focusing on misclassified samples, improving weak learners over multiple rounds.\n",
        "\n",
        "Example: AdaBoost, XGBoost, LightGBM.\n",
        "\n",
        "3.Stacking\n",
        "\n",
        "Uses multiple base models and combines their outputs using a meta-model for better accuracy.\n",
        "\n",
        "Example: Combining Random Forest, SVM, and Neural Networks to make a final decision.\n"
      ],
      "metadata": {
        "id": "ABw9gjIDIBGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###12.What is a Random Forest Classifier?\n",
        "\n",
        "A Random Forest Classifier is an ensemble learning algorithm that combines multiple Decision Trees to improve accuracy, reduce overfitting, and enhance generalization. It is a Bagging-based technique where each tree is trained on a different random subset of the data, and the final prediction is made by majority voting.\n",
        "\n",
        "###How Does a Random Forest Classifier Work?\n",
        "\n",
        "1.Bootstrap Sampling (Bagging)\n",
        "The dataset is randomly split into multiple subsets using bootstrap sampling (sampling with replacement).\n",
        "Each subset is used to train an independent Decision Tree.\n",
        "\n",
        "2.Feature Randomness\n",
        "At each split in a Decision Tree, only a random subset of features is considered.\n",
        "This ensures that trees do not all learn the same patterns, increasing diversity.\n",
        "\n",
        "3. Majority Voting for Prediction\n",
        "For classification, each tree predicts a class, and the most voted class is the final output.\n",
        "\n",
        "For regression, the average of all predictions is taken.\n",
        "\n",
        "###Advantages of Random Forest\n",
        "✔ Reduces Overfitting – Unlike a single Decision Tree, Random Forest generalizes better.\n",
        "\n",
        "✔ Handles Missing Data – It can work well even with some missing values.\n",
        "\n",
        "✔ Works with Large Datasets – Can handle thousands of features without performance loss.\n",
        "\n",
        "✔ Feature Importance – Identifies the most important features for prediction.\n",
        "\n"
      ],
      "metadata": {
        "id": "6kG6o3JGIt-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###13.What are the main types of ensemble techniques?\n",
        "\n",
        "Ensemble learning techniques can be broadly categorized into three main types: Bagging, Boosting, and Stacking. Each method has a different way of combining multiple models to improve accuracy and generalization.\n",
        "\n",
        "1.Bagging (Bootstrap Aggregating)\n",
        "\n",
        "Bagging reduces variance by training multiple models independently on random subsets of the data (bootstrap sampling).\n",
        "The final prediction is obtained by majority voting (classification) or averaging (regression).\n",
        "\n",
        "🔹 Example Algorithms\n",
        "✔ Random Forest (Ensemble of Decision Trees)\n",
        "✔ Bagging Classifier & Regressor\n",
        "\n",
        "✅ Advantages\n",
        "\n",
        "✔ Reduces overfitting (high variance models like Decision Trees).\n",
        "✔ Works well when models are prone to noise.\n",
        "\n",
        "❌ Disadvantages:\n",
        "\n",
        "❌ Computationally expensive (many models are trained).\n",
        "\n",
        "🔧 Example: Bagging Classifier with Decision Trees\n",
        "\n",
        "###2. Boosting\n",
        "Boosting reduces bias by training models sequentially, where each model learns from the mistakes of the previous model.\n",
        "Weak learners are gradually improved by focusing on misclassified samples.\n",
        "🔹 Example Algorithms\n",
        "✔ AdaBoost (Adaptive Boosting)\n",
        "✔ Gradient Boosting (GBM, XGBoost, LightGBM, CatBoost)\n",
        "\n",
        "✅ Advantages:\n",
        "\n",
        "✔ Reduces bias (good for weak models like Decision Trees).\n",
        "\n",
        "✔ Achieves higher accuracy than Bagging in many cases.\n",
        "\n",
        "❌ Disadvantages:\n",
        "\n",
        "❌ More prone to overfitting if not tuned properly.\n",
        "\n",
        "❌ Computationally more expensive than Bagging.\n",
        "\n",
        "###3.Stacking\n",
        "\n",
        "Stacking combines multiple diverse models (base learners) and uses a meta-model to make the final prediction.\n",
        "Unlike Bagging and Boosting, which use the same type of models, Stacking mixes different models (e.g., Decision Trees, SVM, Neural Networks).\n",
        "\n",
        "🔹 Example Algorithms:\n",
        "✔ Stacked Generalization (Combining multiple classifiers with a meta-classifier)\n",
        "\n",
        "✅ Advantages:\n",
        "\n",
        "✔ Can leverage different learning algorithms for better performance.\n",
        "\n",
        "✔ Reduces both bias and variance.\n",
        "\n",
        "❌ Disadvantages:\n",
        "\n",
        "❌ More complex to train and tune.\n",
        "\n",
        "❌ Requires careful selection of base models and meta-model."
      ],
      "metadata": {
        "id": "aT8xfv11JKnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###14.What is ensemble learning in machine learning?\n",
        "\n",
        "Ensemble Learning is a technique in machine learning where multiple models (learners) are combined to improve accuracy, robustness, and generalization. Instead of relying on a single model, ensemble methods aggregate predictions from several models to produce a more reliable output.\n",
        "\n",
        "###Why Use Ensemble Learning?\n",
        "\n",
        "A single model may be prone to overfitting (high variance) or underfitting (high bias).\n",
        "\n",
        "Combining multiple models helps to reduce errors, improve accuracy, and enhance generalization.\n",
        "\n",
        "Works well in complex datasets where a single model might struggle to capture patterns.\n",
        "\n",
        "###How Does Ensemble Learning Work?\n",
        "\n",
        "1.Train Multiple Models\n",
        "\n",
        "Different models are trained on the dataset.\n",
        "\n",
        "2.Aggregate Predictions\n",
        "\n",
        "Predictions are combined using majority voting (classification) or averaging (regression).\n",
        "\n",
        "Final Output\n",
        "The final prediction is more accurate and stable than any individual model.\n",
        "\n",
        "###Types of Ensemble Learning\n",
        "1.Bagging (Bootstrap Aggregating)\n",
        "\n",
        "Goal: Reduce variance and overfitting.\n",
        "\n",
        "How it Works: Multiple models are trained on different random subsets of data, and their predictions are averaged.\n",
        "\n",
        "Example: Random Forest (ensemble of Decision Trees).\n",
        "\n",
        "2. Boosting\n",
        "\n",
        "Goal: Reduce bias by improving weak models.\n",
        "\n",
        "How it Works: Models are trained sequentially, and each model focuses on correcting the mistakes of the previous one.\n",
        "\n",
        "Example: AdaBoost, XGBoost, Gradient Boosting.\n",
        "\n",
        "3. Stacking\n",
        "\n",
        "Goal: Improve accuracy by combining different models.\n",
        "\n",
        "How it Works: Different models make predictions, and a meta-model learns how to best combine these predictions.\n",
        "\n",
        "Example: Combining Random Forest, SVM, and Logistic Regression using a meta-model.\n"
      ],
      "metadata": {
        "id": "Vu2eef1LbwHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###15.When should we avoid using ensemble methods?\n",
        "\n",
        "Although ensemble methods improve accuracy and reduce overfitting, they are not always the best choice. There are certain scenarios where they should be avoided.\n",
        "\n",
        "1️⃣ When Computational Resources Are Limited\n",
        "\n",
        "✔ Why?\n",
        "\n",
        "Ensemble models (especially Boosting and Stacking) require training multiple models, which increases computation time and memory usage.\n",
        "\n",
        "Methods like Random Forest (Bagging) and XGBoost (Boosting) can be slow for large datasets.\n",
        "\n",
        "✔ Alternative:\n",
        "\n",
        "Use simpler models like Logistic Regression, Decision Trees, or SVM if accuracy is sufficient.\n",
        "\n",
        "2️⃣ When a Single Strong Model Is Performing Well\n",
        "\n",
        "✔ Why?\n",
        "\n",
        "If a single model (e.g., Deep Learning, Random Forest) already achieves high accuracy, ensemble learning may not provide significant improvement.\n",
        "\n",
        "Adding complexity without a major accuracy gain wastes resources.\n",
        "\n",
        "✔ Alternative:\n",
        "\n",
        "Use Hyperparameter Tuning to improve the single model instead of ensembling.\n",
        "\n",
        "3️⃣ When Model Interpretability Is Important\n",
        "\n",
        "✔ Why?\n",
        "\n",
        "Ensemble models are often considered black-box models, meaning it's hard to understand why they make certain predictions.\n",
        "\n",
        "In critical fields like medicine, finance, and law, models must be interpretable for decision-making.\n",
        "\n",
        "✔ Alternative:\n",
        "\n",
        "Use Decision Trees, Logistic Regression, or Explainable AI (XAI) techniques.\n",
        "\n",
        "4️⃣ When the Dataset Is Too Small\n",
        "\n",
        "✔ Why?\n",
        "\n",
        "Ensemble methods require diverse data to be effective.\n",
        "\n",
        "On small datasets, models may learn the same patterns, making ensembling redundant.\n",
        "\n",
        "✔ Alternative:\n",
        "\n",
        "Use Cross-Validation with a single model instead of ensembling.\n",
        "\n",
        "5️⃣ When Real-Time Predictions Are Required\n",
        "\n",
        "✔ Why?\n",
        "\n",
        "Ensemble models take longer to make predictions due to multiple models running in parallel or sequence.\n",
        "\n",
        "Applications like self-driving cars, fraud detection, and chatbots need fast inference time.\n",
        "\n",
        "✔ Alternative:\n",
        "\n",
        "Use Lightweight Models like Decision Trees, SVM, or Linear Regression for real-time applications.\n"
      ],
      "metadata": {
        "id": "tkivICk_cQqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###16.How does Bagging help in reducing overfitting?\n",
        "\n",
        "Bagging (Bootstrap Aggregating) reduces overfitting by training multiple models on different random subsets of the training data and then combining their predictions. This helps to lower the variance of the model, making it more generalized to unseen data.\n",
        "\n",
        "###How Bagging Works to Reduce Overfitting\n",
        "\n",
        "1️⃣ Bootstrap Sampling (Random Subsets of Data)\n",
        "\n",
        "Instead of training a single model on the entire dataset, Bagging creates multiple random subsets (with replacement) from the training data.\n",
        "Each subset is used to train a separate model, preventing memorization of noise in the data.\n",
        "\n",
        "2️⃣ Independent Model Training\n",
        "\n",
        "Each model is trained independently, reducing model dependency and preventing overfitting to a particular pattern.\n",
        "\n",
        "3️⃣ Aggregation of Predictions\n",
        "\n",
        "The predictions from all models are combined:\n",
        "\n",
        "For Classification: Majority voting (most frequent class is chosen).\n",
        "For Regression: Averaging (mean of all predictions).\n",
        "\n",
        "This smooths out noise and prevents any single model from dominating the decision, leading to better generalization.\n"
      ],
      "metadata": {
        "id": "809KV75gcUe7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###17.Why is Random Forest better than a single Decision Tree?\n",
        "\n",
        "A Random Forest is better than a single Decision Tree because it improves accuracy, reduces overfitting, and increases stability by combining multiple Decision Trees. Instead of relying on one tree, Random Forest aggregates predictions from multiple trees, making it more robust and generalizable to unseen data.\n",
        "\n",
        "\n",
        "###Why is Random Forest More Powerful?\n",
        "1️⃣ Uses Bagging (Bootstrap Aggregation)\n",
        "\n",
        "Trains multiple trees on random subsets of the data.\n",
        "Reduces variance and prevents overfitting.\n",
        "\n",
        "2️⃣ Feature Randomness\n",
        "\n",
        "At each tree split, it selects a random subset of features, reducing dependency on a few dominant features.\n",
        "This ensures different trees learn different patterns, improving generalization.\n",
        "\n",
        "3️⃣ Majority Voting / Averaging\n",
        "\n",
        "Instead of relying on a single tree, it aggregates predictions from multiple trees:\n",
        "\n",
        "Classification: Uses majority voting (most common class wins).\n",
        "\n",
        "Regression: Uses averaging (smoothens predictions).\n",
        "\n",
        "This results in more stable and accurate predictions.\n"
      ],
      "metadata": {
        "id": "NAA0Ab8kcZKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###18.What is the role of bootstrap sampling in Bagging?\n",
        "\n",
        "Bootstrap sampling is a key technique in Bagging (Bootstrap Aggregating) that helps create multiple diverse training datasets by randomly selecting samples with replacement from the original dataset. It allows each base model to learn from different subsets of data, reducing overfitting and improving stability.\n",
        "\n",
        "###How Does Bootstrap Sampling Work in Bagging?\n",
        "\n",
        "1️⃣ Random Sampling with Replacement\n",
        "\n",
        "Each model in the ensemble is trained on a random subset of the training data.\n",
        "Some data points may appear multiple times, while others may be left out (about 37% of samples are not selected).\n",
        "\n",
        "2️⃣ Training Multiple Models Independently\n",
        "\n",
        "Each subset is used to train a weak learner (e.g., Decision Tree in Random Forest).\n",
        "\n",
        "Since each model sees a different subset, it learns different patterns.\n",
        "\n",
        "3️⃣ Aggregation of Predictions\n",
        "\n",
        "Once all models are trained, their predictions are combined:\n",
        "\n",
        "For Classification: Majority voting (most common class wins).\n",
        "\n",
        "For Regression: Averaging (smoothens predictions).\n",
        "\n",
        "###Why Is Bootstrap Sampling Important?\n",
        "\n",
        "✅ Reduces Overfitting\n",
        "\n",
        "Since each model sees only a portion of the data, it prevents memorization of noise.\n",
        "\n",
        "✅ Creates Diversity Among Models\n",
        "\n",
        "Different models learn different patterns, making the final prediction more generalizable.\n",
        "\n",
        "✅ Improves Stability\n",
        "\n",
        "Models trained on different subsets help prevent bias toward specific features or data points.\n",
        "\n",
        "✅ Allows Out-of-Bag (OOB) Evaluation\n",
        "\n",
        "The 37% of unseen samples can be used to evaluate model performance without a separate validation set.\n",
        "\n"
      ],
      "metadata": {
        "id": "o9F8zfjUceNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###19.What are some real-world applications of ensemble techniques?\n",
        "\n",
        "Ensemble techniques like Bagging, Boosting, and Stacking are widely used in real-world applications to improve accuracy, robustness, and generalization across various industries. Here are some key applications:\n",
        "\n",
        "1️⃣ Fraud Detection (Banking & Finance) 💳\n",
        "\n",
        "Detecting fraudulent transactions in credit cards, loans, and online banking.\n",
        "Boosting techniques like XGBoost and AdaBoost are commonly used to identify suspicious activities.\n",
        "\n",
        "✅ Example:\n",
        "A bank detects credit card fraud by training a Random Forest model on past transaction data. Legitimate vs. fraudulent transactions are classified based on transaction amount, location, and user behavior.\n",
        "\n",
        "\n",
        "2️⃣ Medical Diagnosis & Disease Prediction 🏥\n",
        "\n",
        "Used in cancer detection, heart disease prediction, and drug discovery.\n",
        "Random Forest & Stacking models help improve diagnostic accuracy.\n",
        "\n",
        "✅ Example:\n",
        "A Random Forest model analyzes MRI scans to classify brain tumors as benign or malignant, helping doctors make better treatment decisions.\n",
        "\n",
        "3️⃣ Stock Market Prediction 📈\n",
        "\n",
        "Boosting models like XGBoost and Gradient Boosting analyze historical stock prices and market trends.\n",
        "Used by traders and financial analysts to predict future stock movements.\n",
        "\n",
        "✅ Example:\n",
        "An XGBoost model predicts whether a stock's price will go up or down based on factors like trading volume, historical prices, and market indicators.\n",
        "\n",
        "4️⃣ Spam Email & Malware Detection 📧🔍\n",
        "\n",
        "Used by email providers (Gmail, Outlook) to classify emails as spam or legitimate.\n",
        "Boosting algorithms detect phishing emails and malware.\n",
        "\n",
        "✅ Example:\n",
        "A Spam Classifier uses AdaBoost to identify spam messages based on keywords, sender information, and email formatting.\n",
        "\n",
        "5️⃣ Image Recognition & Object Detection 📷\n",
        "\n",
        "Used in facial recognition, self-driving cars, and medical imaging.\n",
        "\n",
        "Convolutional Neural Networks (CNNs) + Ensemble Models improve accuracy.\n",
        "\n",
        "✅ Example:\n",
        "Self-driving cars use ensemble learning to detect pedestrians, road signs, and other vehicles, ensuring safe navigation.\n",
        "\n",
        "6️⃣ Sentiment Analysis (NLP) 💬\n",
        "\n",
        "Used in social media monitoring, customer reviews, and brand analysis.\n",
        "Stacking models combine LSTMs, Random Forest, and XGBoost for text classification.\n",
        "\n",
        "✅ Example:\n",
        "A Stacked model analyzes Twitter data to determine whether a brand’s sentiment is positive, negative, or neutral.\n",
        "\n",
        "\n",
        "7️⃣ Recommender Systems (E-commerce & Entertainment) 🛒🎬\n",
        "\n",
        "Used by Amazon, Netflix, and YouTube to recommend products, movies, and videos.\n",
        "Hybrid models (Collaborative Filtering + Boosting) personalize recommendations.\n",
        "\n",
        "✅ Example:\n",
        "Netflix uses an ensemble of models to suggest movies based on viewing history, user ratings, and watch time.\n",
        "\n",
        "\n",
        "8️⃣ Healthcare & Drug Discovery 💊\n",
        "\n",
        "Used to predict disease outbreaks and design new medicines.\n",
        "Ensemble learning helps in clinical research and bioinformatics.\n",
        "\n",
        "✅ Example:\n",
        "AI models predict COVID-19 patient risk by analyzing symptoms, X-ray scans, and lab test results using Random Forest + XGBoost.\n",
        "\n",
        "9️⃣ Customer Churn Prediction 📉\n",
        "\n",
        "Used by telecom, banks, and subscription services to predict whether a customer will stop using a service.\n",
        "Boosting models (XGBoost, LightGBM) analyze customer behavior and transaction history.\n",
        "\n",
        "✅ Example:\n",
        "A telecom company predicts which customers are likely to cancel their mobile plans and offers discounts to retain them.\n",
        "\n",
        "\n",
        "10️⃣ Weather Forecasting & Climate Change Analysis 🌦️\n",
        "\n",
        "Used by meteorologists and climate scientists to analyze rainfall, temperature, and natural disasters.\n",
        "Random Forest & Gradient Boosting are used to predict extreme weather conditions.\n",
        "\n",
        "✅ Example:\n",
        "NASA uses ensemble learning to predict hurricanes, tornadoes, and heat waves based on historical climate data.\n",
        "\n",
        "\n",
        "###Conclusion\n",
        "✔ Ensemble techniques are widely used across industries to improve accuracy and decision-making.\n",
        "\n",
        "✔ Bagging (Random Forest), Boosting (XGBoost, AdaBoost), and Stacking power real-world AI applications.\n",
        "\n",
        "✔ Used in finance, healthcare, e-commerce, cybersecurity, weather forecasting, and more.\n"
      ],
      "metadata": {
        "id": "ePWkOTu4chhf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###20.What is the difference between Bagging and Boosting?\n",
        "\n",
        "\n",
        "###1️⃣ Bagging (Bootstrap Aggregating)\n",
        "📌 How It Works:\n",
        "\n",
        "Random subsets of the training data are created using bootstrap sampling (sampling with replacement).\n",
        "\n",
        "Each subset is used to train an independent model (typically Decision Trees).\n",
        "\n",
        "The final prediction is obtained using majority voting (classification) or averaging (regression).\n",
        "\n",
        "📌 Best For:\n",
        "\n",
        "High variance models (like Decision Trees).\n",
        "\n",
        "Reducing overfitting and improving stability.\n",
        "\n",
        "###2️⃣ Boosting\n",
        "📌 How It Works:\n",
        "\n",
        "Models are trained sequentially, where each new model focuses on the mistakes of the previous models.\n",
        "\n",
        "Misclassified instances get higher weights, so the next model pays more attention to them.\n",
        "\n",
        "The final prediction is made by weighted voting (classification) or weighted averaging (regression).\n",
        "\n",
        "📌 Best For:\n",
        "\n",
        "High bias models (like Logistic Regression, Decision Stumps).\n",
        "\n",
        "Improving accuracy on complex datasets.\n",
        "\n",
        "###Key Takeaways:\n",
        "✔ Bagging is useful for high-variance models (e.g., Decision Trees).\n",
        "\n",
        "✔ Boosting is useful for high-bias models that need improvement.\n",
        "\n",
        "✔ Random Forest (Bagging) is good for stability, while XGBoost (Boosting) is good for high accuracy."
      ],
      "metadata": {
        "id": "7UGQ0nQ8cln3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Practical Quetsions"
      ],
      "metadata": {
        "id": "M5IyK7g9gBf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy.\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Decision Trees\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Bagging Classifier Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhaKR70qgFHS",
        "outputId": "288ca96d-b31a-43a3-9ff3-d0212c326986"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22.Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)\n",
        "\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the Diabetes dataset\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Regressor with Decision Trees\n",
        "bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_reg.predict(X_test)\n",
        "\n",
        "# Calculate and print Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Bagging Regressor Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jH9n_ACbgHGp",
        "outputId": "4ea77605-841d-4fe7-f664-e66ce4cec95d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor Mean Squared Error (MSE): 3056.494602247191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23.Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importance = pd.DataFrame({'Feature': feature_names, 'Importance': rf_clf.feature_importances_})\n",
        "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print feature importance scores\n",
        "print(\"Feature Importance Scores:\\n\", feature_importance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzxMFHh5gHDP",
        "outputId": "cc69920d-f0ef-4d7c-eeb7-d827be2dce42"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importance Scores:\n",
            "                     Feature  Importance\n",
            "23               worst area    0.153892\n",
            "27     worst concave points    0.144663\n",
            "7       mean concave points    0.106210\n",
            "20             worst radius    0.077987\n",
            "6            mean concavity    0.068001\n",
            "22          worst perimeter    0.067115\n",
            "2            mean perimeter    0.053270\n",
            "0               mean radius    0.048703\n",
            "3                 mean area    0.047555\n",
            "26          worst concavity    0.031802\n",
            "13               area error    0.022407\n",
            "21            worst texture    0.021749\n",
            "25        worst compactness    0.020266\n",
            "10             radius error    0.020139\n",
            "5          mean compactness    0.013944\n",
            "1              mean texture    0.013591\n",
            "12          perimeter error    0.011303\n",
            "24         worst smoothness    0.010644\n",
            "28           worst symmetry    0.010120\n",
            "16          concavity error    0.009386\n",
            "4           mean smoothness    0.007285\n",
            "19  fractal dimension error    0.005321\n",
            "15        compactness error    0.005253\n",
            "29  worst fractal dimension    0.005210\n",
            "11            texture error    0.004724\n",
            "14         smoothness error    0.004271\n",
            "18           symmetry error    0.004018\n",
            "9    mean fractal dimension    0.003886\n",
            "8             mean symmetry    0.003770\n",
            "17     concave points error    0.003513\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Train a Random Forest Regressor and compare its performance with a single Decision Tree\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the Diabetes dataset\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Single Decision Tree Regressor\n",
        "dt_reg = DecisionTreeRegressor(random_state=42)\n",
        "dt_reg.fit(X_train, y_train)\n",
        "y_pred_dt = dt_reg.predict(X_test)\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Print Mean Squared Errors\n",
        "print(\"Decision Tree Regressor MSE:\", mse_dt)\n",
        "print(\"Random Forest Regressor MSE:\", mse_rf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z23GXEa3gHA5",
        "outputId": "eef2c8ad-e3dd-456f-a730-045342579268"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Regressor MSE: 4976.797752808989\n",
            "Random Forest Regressor MSE: 2952.0105887640448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier with OOB score enabled\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Print the Out-of-Bag (OOB) Score\n",
        "print(\"Out-of-Bag (OOB) Score:\", rf_clf.oob_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kyfmt2zhgG87",
        "outputId": "b06cff24-b278-491e-a3ac-29b9269d2914"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out-of-Bag (OOB) Score: 0.9560439560439561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#26.Train a Bagging Classifier using SVM as a base estimator and print accuracy\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with SVM as the base estimator\n",
        "bagging_clf = BaggingClassifier(estimator=SVC(), n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Bagging Classifier (SVM) Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QByj6VKkgG3j",
        "outputId": "83f35fc5-d70d-4397-824f-b9aef3c3b66b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier (SVM) Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#27.Train a Random Forest Classifier with different numbers of trees and compare accuracy\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define different numbers of trees\n",
        "n_trees = [10, 50, 100, 200]\n",
        "\n",
        "# Train Random Forest Classifiers with different numbers of trees and compare accuracy\n",
        "for n in n_trees:\n",
        "    rf_clf = RandomForestClassifier(n_estimators=n, random_state=42)\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "    y_pred = rf_clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Random Forest with {n} trees - Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFP8bqgKgG0B",
        "outputId": "915dd39c-f36f-410f-a66b-660bea5adb18"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with 10 trees - Accuracy: 1.0\n",
            "Random Forest with 50 trees - Accuracy: 1.0\n",
            "Random Forest with 100 trees - Accuracy: 1.0\n",
            "Random Forest with 200 trees - Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#28.Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Logistic Regression as the base estimator\n",
        "bagging_clf = BaggingClassifier(estimator=LogisticRegression(max_iter=5000), n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make probability predictions on the test set\n",
        "y_prob = bagging_clf.predict_proba(X_test)[:, 1]  # Get probability of the positive class\n",
        "\n",
        "# Calculate and print AUC score\n",
        "auc_score = roc_auc_score(y_test, y_prob)\n",
        "print(\"Bagging Classifier (Logistic Regression) AUC Score:\", auc_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzHR4YsGgGyC",
        "outputId": "ea63f15f-02ae-4543-d65d-8a643f863207"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier (Logistic Regression) AUC Score: 0.99737962659679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#29.Train a Random Forest Regressor and analyze feature importance scores\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Diabetes dataset\n",
        "data = load_diabetes()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importance = pd.DataFrame({'Feature': feature_names, 'Importance': rf_reg.feature_importances_})\n",
        "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print feature importance scores\n",
        "print(\"Feature Importance Scores:\\n\", feature_importance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbcXw1LlhhlZ",
        "outputId": "5eb1d410-5410-446f-816e-a8deeff232e9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importance Scores:\n",
            "   Feature  Importance\n",
            "2     bmi    0.355469\n",
            "8      s5    0.230957\n",
            "3      bp    0.088408\n",
            "9      s6    0.071329\n",
            "0     age    0.058642\n",
            "5      s2    0.057227\n",
            "4      s1    0.052784\n",
            "6      s3    0.051339\n",
            "7      s4    0.024213\n",
            "1     sex    0.009633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#30.Train an ensemble model using both Bagging and Random Forest and compare accuracy\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Bagging Classifier with Decision Trees\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_clf.predict(X_test)\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "# Print accuracy scores\n",
        "print(\"Bagging Classifier Accuracy:\", accuracy_bagging)\n",
        "print(\"Random Forest Classifier Accuracy:\", accuracy_rf)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogwy4OzigGvO",
        "outputId": "d709d013-7456-4601-d5de-962d853f382a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 1.0\n",
            "Random Forest Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#31.Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# GridSearchCV for tuning hyperparameters\n",
        "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters and accuracy\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best accuracy:\", grid_search.best_score_)\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC0kDhbYgGsk",
        "outputId": "d9f320eb-49ee-4ab2-b979-4e3856390a9c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'criterion': 'gini', 'max_depth': None, 'n_estimators': 200}\n",
            "Best accuracy: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#32.Train a Bagging Regressor with different numbers of base estimators and compare performance\n",
        "\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Compare performance for different base estimators\n",
        "n_estimators_list = [10, 50, 100, 200]\n",
        "for n in n_estimators_list:\n",
        "    bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=n, random_state=42)\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"Bagging Regressor with {n} estimators - MSE: {mse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtYopD8RgGpM",
        "outputId": "03258875-6f3b-4a41-81dd-9e92190aa93f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor with 10 estimators - MSE: 3256.961797752809\n",
            "Bagging Regressor with 50 estimators - MSE: 3056.494602247191\n",
            "Bagging Regressor with 100 estimators - MSE: 2970.863235955056\n",
            "Bagging Regressor with 200 estimators - MSE: 2995.6186106741575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#33.Train a Random Forest Classifier and analyze misclassified samples\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_digits(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Identify misclassified samples\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "misclassified = np.where(y_test != y_pred)[0]\n",
        "\n",
        "print(f\"Total Misclassified Samples: {len(misclassified)}\")\n",
        "print(\"Indices of Misclassified Samples:\", misclassified)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2VrkwsvgGmn",
        "outputId": "9305e525-d5f6-4008-9d18-32cf777b444c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Misclassified Samples: 10\n",
            "Indices of Misclassified Samples: [ 97 133 149 159 166 204 234 244 249 339]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "dt_clf.fit(X_train, y_train)\n",
        "dt_accuracy = accuracy_score(y_test, dt_clf.predict(X_test))\n",
        "\n",
        "# Train Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_clf.predict(X_test))\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", dt_accuracy)\n",
        "print(\"Bagging Classifier Accuracy:\", bagging_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwqWa8UpgGkR",
        "outputId": "0c4c1495-95ab-4830-fa4e-4dbf2a51ae87"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#35.Train a Random Forest Classifier and visualize the confusion matrix\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset (Ensure we use the same dataset for training and testing)\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target  # X has 64 features\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, rf_clf.predict(X_test))\n",
        "\n",
        "# Display confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=digits.target_names)\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix - Random Forest\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "gke_rnyLgGhm",
        "outputId": "14393b06-c88b-459c-f6a9-0d42b342be9e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy\n",
        "\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Define base models\n",
        "base_models = [\n",
        "    ('decision_tree', DecisionTreeClassifier(random_state=42)),\n",
        "    ('svm', SVC(probability=True, random_state=42))\n",
        "]\n",
        "\n",
        "# Define meta-model\n",
        "meta_model = LogisticRegression()\n",
        "\n",
        "# Train Stacking Classifier\n",
        "stacking_clf = StackingClassifier(estimators=base_models, final_estimator=meta_model)\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate accuracy\n",
        "stacking_accuracy = accuracy_score(y_test, stacking_clf.predict(X_test))\n",
        "print(\"Stacking Classifier Accuracy:\", stacking_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1M0y-gPlgGes",
        "outputId": "6b54be91-f63c-4e5b-e26b-b06a784ee6b3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#37.Train a Random Forest Classifier and print the top 5 most important features\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names  # Ensure correct feature names are used\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importance = pd.DataFrame({'Feature': feature_names, 'Importance': rf_clf.feature_importances_})\n",
        "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print top 5 most important features\n",
        "print(\"Top 5 Important Features:\\n\", feature_importance.head(5))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_neCEXRWgGbx",
        "outputId": "cd7d616d-727f-4f76-de24-d9c2abcaa5a8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            "                  Feature  Importance\n",
            "23            worst area    0.153892\n",
            "27  worst concave points    0.144663\n",
            "7    mean concave points    0.106210\n",
            "20          worst radius    0.077987\n",
            "6         mean concavity    0.068001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#38.Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdXqKvT5gGY3",
        "outputId": "a5400bec-a3bb-4abd-ce52-23d0474be095"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#39.Train a Random Forest Classifier and analyze the effect of max_depth on accuracy\n",
        "\n",
        "\n",
        "depths = [3, 5, 10, None]\n",
        "for depth in depths:\n",
        "    rf_clf = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42)\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "    accuracy = accuracy_score(y_test, rf_clf.predict(X_test))\n",
        "    print(f\"Random Forest with max_depth={depth} - Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIHfZWnzgGWE",
        "outputId": "af4f0e4a-c189-4d1b-f1e4-44002b1656a2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with max_depth=3 - Accuracy: 1.0\n",
            "Random Forest with max_depth=5 - Accuracy: 1.0\n",
            "Random Forest with max_depth=10 - Accuracy: 1.0\n",
            "Random Forest with max_depth=None - Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#40.Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance.\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "base_estimators = [DecisionTreeRegressor(), KNeighborsRegressor()]\n",
        "for estimator in base_estimators:\n",
        "    bagging_reg = BaggingRegressor(estimator=estimator, n_estimators=50, random_state=42)\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    mse = mean_squared_error(y_test, bagging_reg.predict(X_test))\n",
        "    print(f\"Bagging Regressor with {type(estimator).__name__} - MSE: {mse}\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3D7KPCzgGTg",
        "outputId": "a23e0979-f520-442b-c911-81b4437d79dd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor with DecisionTreeRegressor - MSE: 0.0013733333333333332\n",
            "Bagging Regressor with KNeighborsRegressor - MSE: 0.008722133333333363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#41.Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "\n",
        "# Binarize the labels for multi-class classification\n",
        "n_classes = len(set(y_test))  # Number of unique classes\n",
        "y_test_bin = label_binarize(y_test, classes=np.arange(n_classes))\n",
        "\n",
        "# Get probability predictions for each class\n",
        "y_prob = rf_clf.predict_proba(X_test)\n",
        "\n",
        "# Compute the ROC-AUC score for multi-class (One-vs-Rest)\n",
        "auc_score = roc_auc_score(y_test_bin, y_prob, multi_class='ovr', average='macro')\n",
        "\n",
        "print(\"Random Forest Classifier ROC-AUC Score:\", auc_score)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCn3PgyIgGQz",
        "outputId": "8592e467-ff64-4756-bf01-e13783718f4e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classifier ROC-AUC Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#42.Train a Bagging Classifier and evaluate its performance using cross-validatio\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(bagging_clf, X, y, cv=5, scoring='accuracy')\n",
        "\n",
        "# Print results\n",
        "print(\"Cross-Validation Scores:\", cv_scores)\n",
        "print(\"Mean Accuracy:\", cv_scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvHpPXCWgGOI",
        "outputId": "9254a58d-cfbd-4ad6-99bd-e8f9e4233aec"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation Scores: [0.96666667 0.96666667 0.93333333 0.96666667 1.        ]\n",
            "Mean Accuracy: 0.9666666666666668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#43. Train a Random Forest Classifier and plot the Precision-Recall curve.\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Binarize the labels for multi-class classification\n",
        "n_classes = len(set(y_test))  # Number of classes\n",
        "y_test_bin = label_binarize(y_test, classes=np.arange(n_classes))\n",
        "y_prob_bin = rf_clf.predict_proba(X_test)  # Get probability estimates for each class\n",
        "\n",
        "# Plot Precision-Recall curves for each class\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "for i in range(n_classes):\n",
        "    precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_prob_bin[:, i])\n",
        "    plt.plot(recall, precision, marker='.', label=f'Class {i}')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve (Multiclass)')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "A8QvHUOugGLs",
        "outputId": "6f6ddcf5-4c82-43a3-cd3d-51e26abcfa24"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#44.Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy\n",
        "\n",
        "# Define base models\n",
        "base_models = [\n",
        "    ('random_forest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('logistic_regression', LogisticRegression(max_iter=5000))\n",
        "]\n",
        "\n",
        "# Define meta-model\n",
        "meta_model = LogisticRegression()\n",
        "\n",
        "# Train Stacking Classifier\n",
        "stacking_clf = StackingClassifier(estimators=base_models, final_estimator=meta_model)\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate accuracy\n",
        "stacking_accuracy = accuracy_score(y_test, stacking_clf.predict(X_test))\n",
        "print(\"Stacking Classifier Accuracy:\", stacking_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXlvjNlMgGI5",
        "outputId": "8f22c8ad-4ccd-4649-f251-32fbcec56ca6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#45.Train a Bagging Regressor with different levels of bootstrap samples and compare performance\n",
        "\n",
        "bootstrap_levels = [True, False]\n",
        "\n",
        "for bootstrap in bootstrap_levels:\n",
        "    bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, bootstrap=bootstrap, random_state=42)\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    mse = mean_squared_error(y_test, bagging_reg.predict(X_test))\n",
        "    print(f\"Bagging Regressor with bootstrap={bootstrap} - MSE: {mse}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mr9SnVRjgGGM",
        "outputId": "6b59ae48-c070-47ee-fa85-9fc291eaa60d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor with bootstrap=True - MSE: 0.0013733333333333332\n",
            "Bagging Regressor with bootstrap=False - MSE: 0.0\n"
          ]
        }
      ]
    }
  ]
}